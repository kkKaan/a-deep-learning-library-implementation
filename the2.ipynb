{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIi6vhnurJA-"
      },
      "source": [
        "# THE2 - CENG403 Spring 2024\n",
        "\n",
        "This document contains the backbone structure for the take-home exam. You should complete this template for your solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DwugJ84rTRM"
      },
      "source": [
        "# 1 The Gergen Library [THE1 Solution]\n",
        "\n",
        "This section presents the code for solving THE1. You can modify any part as desired. The implementation of `rastgele_gercek`, `rastgele_dogal`, and `gergen` classes, as well as the necessary operations in the `Operation` class, are all provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXa7Mn78KYNe"
      },
      "source": [
        "## 1.1 Implementation for `rastgele_gercek` and `rastgele_dogal`\n",
        "\n",
        "Uniform distribution is the only option available for `rastgele_dogal`. However, for `rastgele_gercek`, you can choose between uniform and gaussian distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nsyaG2xnv1or"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "from typing import Union\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kI43OQKUTmrV"
      },
      "outputs": [],
      "source": [
        "def cekirdek(sayi: int):\n",
        "    # Sets the seed for random number generation\n",
        "    random.seed(sayi)\n",
        "\n",
        "\n",
        "def rastgele_dogal(boyut, aralik=None, dagilim='uniform'):\n",
        "    \"\"\"\n",
        "    Generates data of specified dimensions with random integer values and returns a gergen object.\n",
        "\n",
        "    Parameters:\n",
        "        boyut (tuple): Shape of the desired data.\n",
        "        aralik (tuple, optional): (min, max) specifying the range of random values. Defaults to None, which implies a default range.\n",
        "        dagilim (string, optional): Distribution of random values ('uniform' or other types). Defaults to 'uniform'.\n",
        "\n",
        "    Returns:\n",
        "        gergen: A new gergen object with random integer values.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set a default range if aralik is not provided\n",
        "    if aralik is None:\n",
        "        aralik = (0, 10)\n",
        "\n",
        "    def generate_random_data(shape):\n",
        "        if len(shape) == 1:\n",
        "            return [random_value(aralik, dagilim) for _ in range(shape[0])]\n",
        "        else:\n",
        "            return [generate_random_data(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    def random_value(aralik, dagilim):\n",
        "        if dagilim == 'uniform':\n",
        "            return random.randint(*aralik)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution: {dagilim}\")\n",
        "\n",
        "    data = generate_random_data(boyut)\n",
        "    return gergen(data)\n",
        "\n",
        "\n",
        "def rastgele_gercek(boyut, aralik=(0.0, 1.0), dagilim='uniform'):\n",
        "    \"\"\"\n",
        "    Generates a gergen of specified dimensions with random floating-point values.\n",
        "\n",
        "    Parameters:\n",
        "        boyut (tuple): Shape of the desired gergen.\n",
        "        aralik (tuple, optional): (min, max) specifying the range of random values. Defaults to (0.0, 1.0) for uniform distribution.\n",
        "        dagilim (string, optional): Distribution of random value (e.g., 'uniform', 'gaussian'). Defaults to 'uniform'.\n",
        "\n",
        "    Returns:\n",
        "        gergen: A new gergen object with random floating-point values.\n",
        "    \"\"\"\n",
        "\n",
        "    def generate_random_data(shape):\n",
        "        if len(shape) == 1:\n",
        "            return [random_value(aralik, dagilim) for _ in range(shape[0])]\n",
        "        else:\n",
        "            return [generate_random_data(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    def random_value(aralik, dagilim):\n",
        "        if dagilim == 'uniform':\n",
        "            return random.uniform(*aralik)\n",
        "        elif dagilim == 'gaussian':\n",
        "            mean, std_dev = aralik\n",
        "            return random.gauss(mean, std_dev)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported distribution: {dagilim}\")\n",
        "\n",
        "    data = generate_random_data(boyut)\n",
        "    return gergen(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoqJr35ILDwi"
      },
      "source": [
        "## 1.2 Operation Class Definition\n",
        "\n",
        "You can find the latest version of the Operation class here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZmQuuHjnwjWa"
      },
      "outputs": [],
      "source": [
        "class Operation:\n",
        "\n",
        "    def __call__(self, *operands, **kwargs):\n",
        "        \"\"\"\n",
        "        Calls the operation with the provided operands and keyword arguments.\n",
        "\n",
        "        Parameters:\n",
        "            *operands: Variable length operand list.\n",
        "            **kwargs: Variable length keyword argument list.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the operation.\n",
        "        \"\"\"\n",
        "        self.operands = operands\n",
        "        self.kwargs = kwargs  # Store keyword arguments separately\n",
        "        self.outputs = None\n",
        "        return self.ileri(*operands, **kwargs)\n",
        "\n",
        "    def ileri(self, *operands, **kwargs):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the operation.\n",
        "        Must be implemented by subclasses to perform the actual operation.\n",
        "\n",
        "        Parameters:\n",
        "            *operands: Variable length operand list.\n",
        "            **kwargs: Variable length keyword argument list.\n",
        "\n",
        "        Raises:\n",
        "            NotImplementedError: If not overridden in a subclass.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Defines the backward pass of the operation.\n",
        "        Must be implemented by subclasses to compute the gradients.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t. the output of this operation.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcmII57SLU4_"
      },
      "source": [
        "## 1.3 Implemented Operations\n",
        "The section contains all implementations from THE1. You can customize any part as you like, and you need to complete the `TODO` sections.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpWNwPiIMjTw"
      },
      "source": [
        "### 1.3.1 Add"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ve5bAhtp7RbR"
      },
      "outputs": [],
      "source": [
        "class Add(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Adds two gergen objects or a gergen object and a scalar.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen or list): The first operand.\n",
        "            b (gergen or list): The second operand.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the addition.\n",
        "        \"\"\"\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a, b]\n",
        "            result = gergen(self.add_gergen(a.duzlestir().listeye(), b.duzlestir().listeye()), operation=self)\n",
        "            result.boyutlandir(a.boyut())\n",
        "        elif isinstance(a, gergen) and isinstance(b, (list)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.add_list(a.listeye(), b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (list)):\n",
        "            self.a = b\n",
        "            self.b = a\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.add_list(b.listeye(), a), operation=self)\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.add_scalar(a.listeye(), b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (int, float)):\n",
        "            self.a = b\n",
        "            self.b = a\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.add_scalar(b.listeye(), a), operation=self)\n",
        "        else:\n",
        "            raise ValueError(\"Add operation requires at least one gergen operand.\")\n",
        "\n",
        "        result.requires_grad = True\n",
        "        return result\n",
        "\n",
        "    def add_scalar(self, a, scalar):\n",
        "        if isinstance(a, list):\n",
        "            return [self.add_scalar(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            return a + scalar\n",
        "\n",
        "    def add_gergen(self, a, b):\n",
        "        # Check if 'a' is a list\n",
        "        if isinstance(a, list):\n",
        "            # Check if 'b' is a list\n",
        "            if isinstance(b, list):\n",
        "                if len(a) != len(b):\n",
        "                    raise ValueError(\"Dimensions of gergen objects do not match for addition.\")\n",
        "                return [a[i] + b[i] for i in range(len(a))]\n",
        "            # If 'a' is a list and 'b' is a scalar\n",
        "            elif not isinstance(b, list):\n",
        "                return [item + b for item in a]\n",
        "\n",
        "        # If 'a' is a scalar and 'b' is a list\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [a + item for item in b]\n",
        "        # Direct addition for scalars, or fallback error for unsupported types\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a + b\n",
        "\n",
        "    def add_list(self, a, b):\n",
        "        # Check if 'a' is a list\n",
        "        if isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.add_list(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # If 'a' is list and b is scalar\n",
        "        elif isinstance(a, list) and not isinstance(b, list):\n",
        "            return [self.add_list(elem_a, b) for elem_a in a]\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.add_list(a, elem_b) for elem_b in b]\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a + b\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        # The gradient with respect to both inputs of addition is 1 if both gergen else the second is 0\n",
        "        grad_a = grad_input\n",
        "        grad_b = grad_input if isinstance(self.b, gergen) else 0\n",
        "        result = (grad_a, grad_b)\n",
        "        return result[:len(self.operands)] # Return derivatives only the number of operands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3krJ5cMmOv"
      },
      "source": [
        "### 1.3.2 Sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nKq0Cvfl7y3a"
      },
      "outputs": [],
      "source": [
        "class Sub(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Subtracts two gergen objects or a gergen object and a scalar.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen or list): The first operand.\n",
        "            b (gergen or list): The second operand.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the subtraction.\n",
        "        \"\"\"\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            self.a, self.b = a, b\n",
        "            self.operands = [a, b]\n",
        "            result = gergen(self.subtract_gergen(a.duzlestir().veri, b.duzlestir().veri), operation=self)\n",
        "            result.boyutlandir(a.boyut())\n",
        "        elif isinstance(a, gergen) and isinstance(b, (list)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.subtract_list(a.veri, b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (list)):\n",
        "            self.a = b\n",
        "            self.b = a\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.subtract_list(a, b.veri), operation=self)\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.subtract_scalar(a.veri, b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (int, float)):\n",
        "            self.a = b\n",
        "            self.b = a\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.subtract_scalar(b.veri, a), operation=self)\n",
        "        else:\n",
        "            raise ValueError(\"Sub operation requires at least one gergen operand.\")\n",
        "        return result\n",
        "\n",
        "    def subtract_scalar(self, a, scalar):\n",
        "        if isinstance(a, list):\n",
        "            return [self.subtract_scalar(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            return a - scalar\n",
        "\n",
        "    def subtract_list(self, a, b):\n",
        "        # Check if 'b' is a list\n",
        "        if isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.subtract_list(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # If 'a' is list and b is scalar\n",
        "        elif isinstance(a, list) and not isinstance(b, list):\n",
        "            return [self.subtract_list(elem_a, b) for elem_a in a]\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [self.subtract_list(a, elem_b) for elem_b in b]\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a - b\n",
        "\n",
        "    def subtract_gergen(self, a, b):\n",
        "        # Check if 'a' is a list\n",
        "        if isinstance(a, list):\n",
        "            # Check if 'b' is a list\n",
        "            if isinstance(b, list):\n",
        "                if len(a) != len(b):\n",
        "                    raise ValueError(\"Dimensions of gergen objects do not match for subtraction.\")\n",
        "                return [a[i] - b[i] for i in range(len(a))]\n",
        "            # If 'a' is a list and 'b' is a scalar\n",
        "            elif not isinstance(b, list):\n",
        "                return [item - b for item in a]\n",
        "        # If 'a' is a scalar and 'b' is a list\n",
        "        elif not isinstance(a, list) and isinstance(b, list):\n",
        "            return [a - item for item in b]\n",
        "        # Direct subtraction for scalars, or fallback error for unsupported types\n",
        "        elif not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a - b\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        # The gradient with respect to the first input is 1, and with respect to the second input is -1\n",
        "        grad_a = grad_input\n",
        "        grad_b = -grad_input if isinstance(self.b, gergen) else 0\n",
        "        result = (grad_a, grad_b)\n",
        "        return result[:len(self.operands)] # Return derivatives only the number of operands"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHPjWN-2NsjH"
      },
      "source": [
        "### 1.3.3 TrueDiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZhFVRBbP83oC"
      },
      "outputs": [],
      "source": [
        "class TrueDiv(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Divides two gergen objects or a gergen object and a scalar.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen or list): The first operand.\n",
        "            b (gergen or list): The second operand.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the division.\n",
        "        \"\"\"\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            self.a, self.b = a, b\n",
        "            self.operands = [a, b]\n",
        "            result = gergen(self.divide_elements(a.duzlestir().veri, b.duzlestir().veri), operation=self)\n",
        "            result.boyutlandir(a.boyut())\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.divide_scalar(a.veri, b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (int, float)):\n",
        "            # Division of a scalar by a gergen object is not typically defined,\n",
        "            # but you can implement it based on your requirements.\n",
        "            raise NotImplementedError(\"Division of a scalar by a gergen object is not implemented.\")\n",
        "        else:\n",
        "            raise ValueError(\"TrueDiv operation requires at least one gergen operand.\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    def divide_scalar(self, a, scalar):\n",
        "        if isinstance(a, list):\n",
        "            return [self.divide_scalar(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            if scalar == 0:\n",
        "                raise ZeroDivisionError(\"Division by zero.\")\n",
        "            return a / scalar\n",
        "\n",
        "    def divide_elements(self, a, b):\n",
        "        # Both a and b are non-lists (scalars), perform direct division\n",
        "        if not isinstance(a, list) and not isinstance(b, list):\n",
        "            if b == 0:\n",
        "                raise ZeroDivisionError(\"Division by zero.\")\n",
        "            return a / b\n",
        "        # Both a and b are lists, perform element-wise division\n",
        "        elif isinstance(a, list) and isinstance(b, list):\n",
        "            if len(a) != len(b):\n",
        "                raise ValueError(\"Dimensions of gergen objects do not match for division.\")\n",
        "            return [self.divide_elements(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # One of a or b is a list and the other is a scalar, divide each element of the list by the scalar\n",
        "        elif isinstance(a, list):\n",
        "            return [self.divide_elements(elem, b) for elem in a]\n",
        "        else:\n",
        "            raise NotImplementedError(\"Division of scalar by a list is not typically defined.\")\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the division operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Gradients w.r.t each operand.\n",
        "        \"\"\"\n",
        "        a, b = self.a, self.b\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            # Gradient w.r.t a is grad_input / b\n",
        "            grad_a = grad_input / b  # Assuming grad_input is not a scalar\n",
        "            # Gradient w.r.t b is -grad_input * a / (b ** 2)\n",
        "            grad_b = (-grad_input * a) / (b.us(2))\n",
        "            return grad_a, grad_b\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            # When b is a scalar, the gradient w.r.t a is grad_input / b\n",
        "            grad_a = grad_input / b  # Assuming grad_input is not a scalar\n",
        "            # There is no gradient w.r.t to a scalar in the context of training neural networks\n",
        "            return grad_a\n",
        "        else:\n",
        "            raise NotImplementedError(\n",
        "                \"Backpropagation for the division of a scalar by a gergen object is not supported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIAye1-nNvW7"
      },
      "source": [
        "### 1.3.4 Mul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4JKZjx6v_EzM"
      },
      "outputs": [],
      "source": [
        "class Mul(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Multiplies two gergen objects or a gergen object and a scalar.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen or list): The first operand.\n",
        "            b (gergen or list): The second operand.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the multiplication.\n",
        "        \"\"\"\n",
        "        if isinstance(a, gergen) and isinstance(b, gergen):\n",
        "            self.a, self.b = a, b\n",
        "            self.operands = [a, b]\n",
        "            # a is a scalar gergen\n",
        "            if a.uzunluk() == 1:\n",
        "                result = gergen(self.multiply_scalar(b.veri, a.veri), operation=self)\n",
        "            # b is a scalar gergen\n",
        "            elif b.uzunluk() == 1:\n",
        "                result = gergen(self.multiply_scalar(a.veri, b.veri), operation=self)\n",
        "            else:\n",
        "                result = gergen(self.multiply_elements(a.duzlestir().veri,\n",
        "                                                       b.duzlestir().veri),\n",
        "                                operation=self)\n",
        "                result.boyutlandir(a.boyut())\n",
        "        elif isinstance(a, gergen) and isinstance(b, (int, float)):\n",
        "            self.a = a\n",
        "            self.b = b\n",
        "            self.operands = [a]\n",
        "            result = gergen(self.multiply_scalar(a.veri, b), operation=self)\n",
        "        elif isinstance(b, gergen) and isinstance(a, (int, float)):\n",
        "            self.a = b\n",
        "            self.b = a\n",
        "            self.operands = [b]\n",
        "            result = gergen(self.multiply_scalar(b.veri, a), operation=self)\n",
        "        else:\n",
        "            raise ValueError(\"Mul operation requires at least one gergen operand.\")\n",
        "\n",
        "        result.requires_grad = True\n",
        "        return result\n",
        "\n",
        "    def multiply_scalar(self, a, scalar):\n",
        "        if isinstance(a, list):\n",
        "            return [self.multiply_scalar(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            return a * scalar\n",
        "\n",
        "    def multiply_elements(self, a, b):\n",
        "        # Both a and b are non-lists (scalars), perform direct multiplication\n",
        "        if not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a * b\n",
        "        # Both a and b are lists, perform element-wise multiplication\n",
        "        elif isinstance(a, list) and isinstance(b, list):\n",
        "            if len(a) != len(b):\n",
        "                raise ValueError(\"Dimensions of gergen objects do not match for multiplication.\")\n",
        "            return [self.multiply_elements(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # One of a or b is a list and the other is a scalar, multiply each element of the list by the scalar\n",
        "        elif isinstance(a, list):\n",
        "            return [self.multiply_elements(elem, b) for elem in a]\n",
        "        else:\n",
        "            return [self.multiply_elements(a, elem) for elem in b]\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the multiplication operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Gradients w.r.t each operand.\n",
        "        \"\"\"\n",
        "        a, b = self.a, self.b\n",
        "        grad_a = grad_input * b\n",
        "        grad_b = grad_input * a if isinstance(b, gergen) else 0\n",
        "        result = (grad_a, grad_b)\n",
        "        return result[:len(self.operands)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0MM37TQOAoj"
      },
      "source": [
        "### 1.3.5 Us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aXS2PFPtBKZ2"
      },
      "outputs": [],
      "source": [
        "class Us(Operation):\n",
        "\n",
        "    def ileri(self, a, n):\n",
        "        \"\"\"\n",
        "        Power operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The base.\n",
        "            n (int): The exponent.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the power operation.\n",
        "        \"\"\"\n",
        "        self.a = a\n",
        "        self.n = n\n",
        "        self.operands = [a]\n",
        "        result = gergen(self.power_elements(a.veri, n), operation=self)\n",
        "        return result\n",
        "\n",
        "    def power_elements(self, a, n):\n",
        "        if isinstance(a, list):\n",
        "            return [self.power_elements(elem, n) for elem in a]\n",
        "        else:\n",
        "            return a**n\n",
        "\n",
        "    def multiply_elements(self, a, b):\n",
        "        # Both a and b are non-lists (scalars), perform direct multiplication\n",
        "        if not isinstance(a, list) and not isinstance(b, list):\n",
        "            return a * b\n",
        "        # Both a and b are lists, perform element-wise multiplication\n",
        "        elif isinstance(a, list) and isinstance(b, list):\n",
        "            if len(a) != len(b):\n",
        "                raise ValueError(\"Dimensions of gergen objects do not match for multiplication.\")\n",
        "            return [self.multiply_elements(elem_a, elem_b) for elem_a, elem_b in zip(a, b)]\n",
        "        # One of a or b is a list and the other is a scalar, multiply each element of the list by the scalar\n",
        "        elif isinstance(a, list):\n",
        "            return [self.multiply_elements(elem, b) for elem in a]\n",
        "        else:\n",
        "            return [self.multiply_elements(a, elem) for elem in b]\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the power operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            Tuple: Gradients w.r.t each operand.\n",
        "        \"\"\"\n",
        "        a, n = self.a, self.n\n",
        "        grad_a = grad_input * n * (a.us(n - 1))\n",
        "        return grad_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzFMcdJtOHzg"
      },
      "source": [
        "### 1.3.6 Log10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_zQWrw_zFHs3"
      },
      "outputs": [],
      "source": [
        "class Log10(Operation):\n",
        "\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Log10 operation\n",
        "\n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the log10 operation.\n",
        "        \"\"\"\n",
        "        self.a = a\n",
        "        self.operands = [a]\n",
        "        # Recursively check for non-positive values in the nested list structure\n",
        "        if self.contains_non_positive(self.a.veri):\n",
        "            raise ValueError(\"Logarithm undefined for non-positive values.\")\n",
        "        result = gergen(self.log_elements(a.veri), operation=self)\n",
        "        return result\n",
        "\n",
        "    def log_elements(self, a):\n",
        "        # Recursively apply the base 10 logarithm to each element\n",
        "        if isinstance(a, list):\n",
        "            return [self.log_elements(elem) for elem in a]\n",
        "        else:\n",
        "            return math.log10(a)\n",
        "\n",
        "    def contains_non_positive(self, a):\n",
        "        # Recursively check for non-positive values and flatten the results\n",
        "        def check_and_flatten(a):\n",
        "            flag = False\n",
        "            if isinstance(a, list):\n",
        "                # Use a generator expression to recursively check each element and flatten the result\n",
        "                for ele in a:\n",
        "                    flag = check_and_flatten(ele)\n",
        "            else:\n",
        "                if a <= 0:\n",
        "                    return True\n",
        "            return flag\n",
        "\n",
        "        # Use 'any' on a flattened generator of boolean values\n",
        "        return check_and_flatten(a)\n",
        "\n",
        "    def multiply_elements(self, a, scalar):\n",
        "        # Recursively multiply each element by the scalar\n",
        "        if isinstance(a, list):\n",
        "            return [self.multiply_elements(elem, scalar) for elem in a]\n",
        "        else:\n",
        "            return a * scalar\n",
        "\n",
        "    def divide_elements(self, grad_output, b):\n",
        "        # Recursively divide grad_output by b, assuming they have the same structure\n",
        "        if isinstance(b, list):\n",
        "            return [self.divide_elements(elem_grad, elem_b) for elem_grad, elem_b in zip(grad_output, b)]\n",
        "        else:\n",
        "            return grad_output / b\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Log10 operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The gradient of the loss w.r.t the input of this operation.\n",
        "        \"\"\"\n",
        "        a = self.a\n",
        "        # Calculate the gradient w.r.t a using the properties of logarithmic differentiation\n",
        "        # Gradient w.r.t a is: grad_input * (1 / (a * ln(10)))\n",
        "        ln10 = math.log(10)\n",
        "        grad_a = grad_input * (1 / (a * ln10))\n",
        "        return grad_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaw1gxIGOVT9"
      },
      "source": [
        "### 1.3.7 Ln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sUo415Lo7q07"
      },
      "outputs": [],
      "source": [
        "class Ln(Operation):\n",
        "\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Implements the forward pass for the Ln operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the natural logarithm operation.\n",
        "        \"\"\"\n",
        "        if not isinstance(a, gergen):\n",
        "            raise ValueError(\"Ln operation requires a gergen operand.\")\n",
        "        self.a = a\n",
        "        self.operands = [a]\n",
        "        if self.contains_non_positive(self.a.listeye()):\n",
        "            raise ValueError(\"Logarithm undefined for non-positive values.\")\n",
        "\n",
        "        result = gergen(self.log_elements(a.listeye()), operation=self)\n",
        "        return result\n",
        "\n",
        "    def log_elements(self, a):\n",
        "        # Recursively apply the base 10 logarithm to each element\n",
        "        if isinstance(a, list):\n",
        "            return [self.log_elements(elem) for elem in a]\n",
        "        else:\n",
        "            return math.log(a) if a > 0 else math.log(a + 10**-4)\n",
        "\n",
        "    def contains_non_positive(self, a):\n",
        "        # Recursively check for non-positive values\n",
        "        def check_and_flatten(a):\n",
        "            if isinstance(a, list):\n",
        "                return any(check_and_flatten(elem) for elem in a)\n",
        "            else:\n",
        "                if a <= 0:\n",
        "                    a = 1\n",
        "                    return True\n",
        "                else:\n",
        "                    return False\n",
        "\n",
        "        # Use 'any' on a flattened generator of boolean values\n",
        "        return check_and_flatten(a)\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Ln operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input: The gradient of the loss w.r.t the output of this operation.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The gradient of the loss w.r.t the input of this operation.\n",
        "        \"\"\"\n",
        "        a = self.a\n",
        "        # Calculate the gradient w.r.t a using the properties of logarithmic differentiation\n",
        "        # Gradient w.r.t a is: grad_input * (1 / a)\n",
        "        grad_a = grad_input / a\n",
        "        return grad_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ij6-IOwKOYEw"
      },
      "source": [
        "### 1.3.8 Sin, Cos, Tan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tgY1stEKtflW"
      },
      "outputs": [],
      "source": [
        "def apply_elementwise(g, func):\n",
        "    \"\"\"\n",
        "    Applies a given function element-wise to the data in a gergen object.\n",
        "    This version is capable of handling nested lists of any depth.\n",
        "\n",
        "    Parameters:\n",
        "        g (gergen): The input gergen object.\n",
        "        func (function): The function to apply to the data.\n",
        "    \n",
        "    Returns:\n",
        "        list: A new veri for a gergen object with the function applied element-wise.\n",
        "    \"\"\"\n",
        "\n",
        "    def recursive_apply(data):\n",
        "        if isinstance(data, list):\n",
        "            # Recursively apply func to each element if data is a list\n",
        "            return [recursive_apply(sublist) for sublist in data]\n",
        "        else:\n",
        "            # Apply func directly if data is a scalar (non-list)\n",
        "            return func(data)\n",
        "\n",
        "    # Use the recursive function to apply the operation to the gergen object's data\n",
        "    return recursive_apply(g.listeye())\n",
        "\n",
        "\n",
        "class Sin(Operation):\n",
        "\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Implements the forward pass for the Sin operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the sine operation.\n",
        "        \"\"\"\n",
        "        self.operands = [a]\n",
        "        result = gergen(apply_elementwise(a, math.sin), operation=self)\n",
        "        return result\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Sin operation.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        # The gradient with respect to a is grad_output multiplied by the derivative of sin(a), which is cos(a).\n",
        "        cos_a = apply_elementwise(a, math.cos)\n",
        "        grad_a = grad_output * cos_a\n",
        "        return grad_a\n",
        "\n",
        "\n",
        "class Cos(Operation):\n",
        "\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Implements the forward pass for the Cos operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the cosine operation.\n",
        "        \"\"\"\n",
        "        self.operands = [a]\n",
        "        result = gergen(apply_elementwise(a, math.cos), operation=self)\n",
        "        return result\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Cos operation.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        # The gradient with respect to a is grad_output multiplied by the derivative of cos(a), which is -sin(a).\n",
        "        sin_a = apply_elementwise(a, math.sin)\n",
        "        grad_a = grad_output * -sin_a\n",
        "        return grad_a\n",
        "\n",
        "\n",
        "class Tan(Operation):\n",
        "\n",
        "    def ileri(self, a):\n",
        "        \"\"\"\n",
        "        Implements the forward pass for the Tan operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the tangent operation.\n",
        "        \"\"\"\n",
        "        self.operands = [a]\n",
        "        result = gergen(apply_elementwise(a, math.tan), operation=self)\n",
        "        return result\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Tan operation.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        # The gradient with respect to a is grad_output multiplied by the derivative of tan(a), which is sec^2(a).\n",
        "        sec2_a = apply_elementwise(a, lambda x: 1 / math.cos(x)**2)\n",
        "        grad_a = grad_output * sec2_a\n",
        "        return grad_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coWq1h40OhFz"
      },
      "source": [
        "### 1.3.9 Topla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "67jdQOqIbOSv"
      },
      "outputs": [],
      "source": [
        "class Topla(Operation):\n",
        "\n",
        "    def ileri(self, a, eksen=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the Topla operation.\n",
        "        \n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "            eksen (int, optional): The axis along which to sum the elements. Defaults to None.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: The result of the sum operation.\n",
        "        \"\"\"\n",
        "\n",
        "        def sum_elements(lst):\n",
        "            if isinstance(lst[0], list):\n",
        "                return [sum_elements(sublst) for sublst in zip(*lst)]\n",
        "            else:\n",
        "                return sum(lst)\n",
        "\n",
        "        def sum_along_axis(data, axis):\n",
        "            if axis == 0:\n",
        "                return sum_elements(data)\n",
        "            else:\n",
        "                return [sum_along_axis(subdata, axis - 1) for subdata in data]\n",
        "\n",
        "        self.operands = [a]\n",
        "        self.eksen = eksen\n",
        "        if eksen is None:\n",
        "            result = sum(a.duzlestir().listeye())\n",
        "        elif isinstance(eksen, int):\n",
        "            if eksen < 0 or eksen >= len(a.boyut()):\n",
        "                raise ValueError(\"Axis out of bounds for gergen's dimensionality\")\n",
        "            result = sum_along_axis(a.listeye(), eksen)\n",
        "        else:\n",
        "            raise TypeError(\"Axis must be an integer or None\")\n",
        "\n",
        "        return gergen(result, operation=self)\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        (Optional, not tested) Computes the backward pass of the Topla operation.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        if self.eksen is None:\n",
        "            # If the sum was across the entire tensor, every element contributes equally.\n",
        "            grad_input_shape = [1] * len(a.boyut())  # Create a shape of ones\n",
        "            expanded_grad = grad_output.boyutlandir(grad_input_shape)  # Expand grad to match input shape\n",
        "            grad_input = expanded_grad * gergen.custom_zeros(a.boyut())  # Multiply by a tensor of ones\n",
        "        else:\n",
        "            # If sum was along a particular axis, replicate the gradient along that axis\n",
        "            repeats = [1] * len(a.boyut())\n",
        "            repeats[self.eksen] = a.boyut()[self.eksen]\n",
        "            grad_input = grad_output.repeat(repeats)\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIxz7MXOjUo"
      },
      "source": [
        "### 1.3.10 Ortalama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "JHkgvui1bJLq"
      },
      "outputs": [],
      "source": [
        "class Ortalama(Operation):\n",
        "\n",
        "    def ileri(self, a, eksen=None):\n",
        "        \"\"\"\n",
        "        Forward pass for the Ortalama operation.\n",
        "\n",
        "        Parameters:\n",
        "            a (gergen): The input gergen object.\n",
        "            eksen (int, optional): The axis along which to compute the average. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the average operation.\n",
        "        \"\"\"\n",
        "\n",
        "        def average_elements(total_sum, total_elements):\n",
        "            # Compute the average\n",
        "            if isinstance(total_sum, list):\n",
        "                # If total_sum is a list (multi-dimensional case), calculate the average for each sublist\n",
        "                return [average_elements(ts, total_elements) for ts in total_sum]\n",
        "            else:\n",
        "                # For a single number, just divide\n",
        "                return total_sum / total_elements\n",
        "\n",
        "        self.operands = [a]\n",
        "        self.eksen = eksen\n",
        "        sum_op = Topla()  # Instantiate the Sum operation\n",
        "\n",
        "        total_sum = sum_op.ileri(a, eksen=eksen).listeye()\n",
        "\n",
        "        if eksen is None:\n",
        "            total_elements = a.uzunluk()\n",
        "        else:\n",
        "            if eksen < 0 or eksen >= len(a.boyut()):\n",
        "                raise ValueError(\"Axis out of bounds for gergen's dimensionality\")\n",
        "            total_elements = a.boyut()[eksen]\n",
        "\n",
        "        # Compute the average\n",
        "        average_result = average_elements(total_sum, total_elements)\n",
        "        return gergen(average_result, operation=self)\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the Ortalama operation. ?????\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "\n",
        "        # Calculate the number of elements involved in the averaging operation.\n",
        "        if self.eksen is None:\n",
        "            # Average over all elements\n",
        "            total_elements = a.uzunluk()\n",
        "        else:\n",
        "            # Average over a specific axis\n",
        "            total_elements = a.boyut()[len(a.boyut()) - self.eksen - 1]\n",
        "\n",
        "        # The gradient of the average is the incoming gradient divided by the number of elements.\n",
        "        # This division gives us the gradient per element that was averaged.\n",
        "        # We then need to create a tensor with this gradient for each element that was part of the average.\n",
        "\n",
        "        # First, create a gergen object with the same shape as 'a', filled with 1/total_elements\n",
        "        grad_per_element = gergen(1 / total_elements)\n",
        "\n",
        "        # Then, we'll broadcast this gradient to the shape of 'a' to distribute it across all elements.\n",
        "        # Multiplication with the broadcasted gradient tensor gives us the gradient with respect to the input 'a'.\n",
        "        grad_a = grad_output * grad_per_element\n",
        "\n",
        "        return grad_a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDp86vtgOnH8"
      },
      "source": [
        "### 1.3.11 IcCarpim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "744_ff3uynvW"
      },
      "outputs": [],
      "source": [
        "class IcCarpim(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Computes the dot product of two gergen objects.\n",
        "\n",
        "        Parameters:\n",
        "            a (gergen): The first operand.\n",
        "            b (gergen): The second operand.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the dot product operation.\n",
        "        \"\"\"\n",
        "        self.a = a\n",
        "        self.b = b\n",
        "        self.operands = [a, b]\n",
        "        if not isinstance(a, type(b)):\n",
        "            raise ValueError(\"Both operands must be gergen objects.\")\n",
        "\n",
        "        def is_vector(v):\n",
        "            return len(v.boyut()) == 1\n",
        "\n",
        "        def is_matrix(m):\n",
        "            return len(m.boyut()) == 2\n",
        "\n",
        "        def vector_dot_product(v1, v2):\n",
        "            if len(v1) != len(v2):\n",
        "                raise ValueError(\"Vectors must have the same length for dot product.\")\n",
        "            return sum(x * y for x, y in zip(v1, v2))\n",
        "\n",
        "        def matrix_multiply(m1, m2):\n",
        "            if len(m1[0]) != len(m2):\n",
        "                raise ValueError(\n",
        "                    \"The number of columns in the first matrix must match the number of rows in the second matrix.\"\n",
        "                )\n",
        "            return [[sum(a * b for a, b in zip(row_a, col_b)) for col_b in zip(*m2)] for row_a in m1]\n",
        "\n",
        "        if len(a.boyut()) > 2 or len(b.boyut()) > 2:\n",
        "            raise ValueError(\"Operands must both be either 1-D vectors or 2-D matrices.\")\n",
        "        elif is_vector(a) and is_vector(b):\n",
        "            # Perform vector dot product\n",
        "            result = vector_dot_product(a.listeye(), b.listeye())\n",
        "        elif is_matrix(a) and is_matrix(b):\n",
        "            # Perform matrix multiplication\n",
        "            result = matrix_multiply(a.listeye(), b.listeye())\n",
        "        else:\n",
        "            raise ValueError(\"Operands must both be either 1-D vectors or 2-D matrices.\")\n",
        "\n",
        "        # Return result\n",
        "        return gergen(result, operation=self, requires_grad=True)\n",
        "\n",
        "    def geri(self, grad_output):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the IcCarpim operation.\n",
        "\n",
        "        Parameters:\n",
        "            grad_output: The gradient of the loss w.r.t the output of this operation.\n",
        "        \n",
        "        Returns:\n",
        "            Tuple: Gradients w.r.t each operand.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        b = self.operands[1]\n",
        "\n",
        "        # Ensure the gradient output is a gergen object\n",
        "        if not isinstance(grad_output, gergen):\n",
        "            grad_output = gergen(grad_output)\n",
        "\n",
        "        # Compute gradients with respect to inputs\n",
        "        if len(a.boyut()) == 2 and len(b.boyut()) == 2:\n",
        "            # Matrix multiplication case\n",
        "            grad_a = grad_output.ic_carpim(b.devrik())  # grad_output (m x p) * b^T (p x n) => (m x n)\n",
        "            grad_b = a.devrik().ic_carpim(grad_output)  # a^T (n x m) * grad_output (m x p) => (n x p)\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"Operands and gradient outputs must both be 2-D matrices for matrix multiplication.\")\n",
        "\n",
        "        return grad_a, grad_b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZIOTmN-OxHS"
      },
      "source": [
        "### 1.3.12 DisCarpim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nzBmeik1Q9yc"
      },
      "outputs": [],
      "source": [
        "class DisCarpim(Operation):\n",
        "\n",
        "    def ileri(self, a, b):\n",
        "        \"\"\"\n",
        "        Computes the outer product of two gergen objects.\n",
        "\n",
        "        Parameters:\n",
        "            a (gergen): The first operand.\n",
        "            b (gergen): The second operand.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The result of the outer product operation.\n",
        "        \"\"\"\n",
        "        if not isinstance(a, gergen) or not isinstance(b, gergen):\n",
        "            raise ValueError(\"Both operands must be gergen objects.\")\n",
        "\n",
        "        # Ensure the veri attributes are lists representing vectors\n",
        "        if not all(isinstance(x, (int, float))\n",
        "                   for x in a.listeye()) or not all(isinstance(y, (int, float)) for y in b.listeye()):\n",
        "            raise ValueError(\"Both gergen objects must contain 1-D numerical data.\")\n",
        "\n",
        "        self.operands = [a, b]\n",
        "        # Compute the outer product\n",
        "        result = [[x * y for y in b.listeye()] for x in a.listeye()]\n",
        "\n",
        "        # Return a new gergen object with the outer product as its veri\n",
        "        return gergen(result, operation=self)\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the DisCarpim operation.\n",
        "        \"\"\"\n",
        "        a = self.operands[0]\n",
        "        b = self.operands[1]\n",
        "\n",
        "        # Gradients for a are the dot product of grad_input with each column of b\n",
        "        # grad_input.shape = (len(a.veri), len(b.veri))\n",
        "        # We sum over the columns of grad_input for each element in b\n",
        "        grad_a = [\n",
        "            sum(grad_input.veri[i][j] * b.veri[j] for j in range(len(b.veri))) for i in range(len(a.veri))\n",
        "        ]\n",
        "\n",
        "        # Gradients for b are the dot product of grad_input with each row of a\n",
        "        # We sum over the rows of grad_input for each element in a\n",
        "        grad_b = [\n",
        "            sum(grad_input.veri[i][j] * a.veri[i] for i in range(len(a.veri))) for j in range(len(b.veri))\n",
        "        ]\n",
        "\n",
        "        # Return gradients wrapped in gergen objects\n",
        "        return gergen(grad_a, operation=self), gergen(grad_b, operation=self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFDvrE8pPyTT"
      },
      "source": [
        "### 1.3.13 Gergen Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oJ76D4AIO2q9"
      },
      "outputs": [],
      "source": [
        "class gergen:\n",
        "    __veri = None  # A nested list of numbers representing the data\n",
        "    D = None  # Transpose of data\n",
        "    turev = None  # Stores the derivate\n",
        "    operation = None  # Stores the operation that produced the gergen\n",
        "    __boyut = None  # Dimensions of the gergen (Shape)\n",
        "    requires_grad = True  # Flag to determine if the gradient should be computed\n",
        "\n",
        "    def __init__(self, veri=None, operation=None, requires_grad=True):\n",
        "        # The constructor for the 'gergen' class.\n",
        "        if veri is None:\n",
        "            self.__veri = []\n",
        "            self.__boyut = (0,)\n",
        "            self.D = None\n",
        "            self.turev = None\n",
        "            self.operation = operation\n",
        "            self.requires_grad = requires_grad\n",
        "        else:\n",
        "            self.__veri = veri\n",
        "            self.__boyut = self.get_shape(veri, ())  # Assuming rectangular data\n",
        "            self.D = None\n",
        "            self.turev = None\n",
        "            self.operation = operation\n",
        "            self.requires_grad = requires_grad\n",
        "\n",
        "    def __iter__(self):\n",
        "        # The __iter__ method returns the iterator object itself.\n",
        "        # You can reset the iterator here if you want to allow multiple passes over the data.\n",
        "\n",
        "        # Reset the iterator\n",
        "        self.__current = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # The __next__ method should return the next value from the iterator.\n",
        "\n",
        "        # Check if we've reached the end of the list\n",
        "        if self.__current < len(self.__veri):\n",
        "            result = self.__veri[self.__current]\n",
        "            self.__current += 1\n",
        "            return result\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        \"\"\"\n",
        "        Allows for indexing or slicing the gergen object's data.\n",
        "\n",
        "        Parameters:\n",
        "            key (int, slice, tuple): An integer or slice for one-dimensional indexing,\n",
        "                                     or a tuple for multi-dimensional indexing/slicing.\n",
        "\n",
        "        Returns:\n",
        "            The element or a new gergen object corresponding to the provided key.\n",
        "        \"\"\"\n",
        "\n",
        "        # Helper function to handle recursive indexing/slicing\n",
        "        def index_or_slice(data, key):\n",
        "            if isinstance(key, int) or isinstance(key, slice):\n",
        "                return data[key]\n",
        "            elif isinstance(key, tuple):\n",
        "                result = data\n",
        "                for k in key:\n",
        "                    result = index_or_slice(result, k)\n",
        "                return result\n",
        "            else:\n",
        "                raise TypeError(f\"Invalid index type: {type(key)}\")\n",
        "\n",
        "        # Perform the indexing or slicing operation\n",
        "        result = index_or_slice(self.__veri, key)\n",
        "        # If the result is a list, return it wrapped in a new gergen object\n",
        "        return gergen(result)\n",
        "\n",
        "    def __str__(self):\n",
        "        # Generates a string representation\n",
        "        if self.uzunluk() == 0:\n",
        "            return \"Empty Gergen\"\n",
        "        else:\n",
        "            shape_str = \"\"\n",
        "            for b in self.boyut():\n",
        "                shape_str += str(b) + \"x\"\n",
        "            if shape_str == \"\":\n",
        "                shape_str += \"0x\"\n",
        "            return shape_str[:-1] + \" boyutlu gergen:\" + \"\\n\" + self.str_helper(\n",
        "                self.listeye(), len(self.boyut()))\n",
        "\n",
        "    def str_helper(self, data, shape, depth=0):\n",
        "        if not shape:\n",
        "            return str(data)\n",
        "        elif not isinstance(data[0], list):\n",
        "            return str(data)\n",
        "        else:\n",
        "            inner_results = []\n",
        "            for subdata in data:\n",
        "                inner_results.append(self.str_helper(subdata, shape, depth + 1))\n",
        "\n",
        "            result = \"[\" + (\"\\n\" * (shape - depth - 1)).join(r for r in inner_results) + \"]\"\n",
        "            return result\n",
        "\n",
        "    @property\n",
        "    def veri(self):\n",
        "        return self.__veri\n",
        "\n",
        "    @staticmethod\n",
        "    def get_shape(lst, shape=()):\n",
        "        if not isinstance(lst, list):\n",
        "            # base case\n",
        "            return shape\n",
        "        # peek ahead and assure all lists in the next depth\n",
        "        # have the same length\n",
        "        if isinstance(lst[0], list):\n",
        "            l = len(lst[0])\n",
        "            if not all(len(item) == l for item in lst):\n",
        "                msg = 'not all lists have the same length'\n",
        "                raise ValueError(msg)\n",
        "\n",
        "        shape += (len(lst),)\n",
        "        # recurse\n",
        "        shape = gergen.get_shape(lst[0], shape)\n",
        "\n",
        "        return shape\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_zeros(shape):\n",
        "        \"\"\"\n",
        "        Creates a multi-dimensional array of zeros with the specified shape.\n",
        "\n",
        "        Parameters:\n",
        "            shape (tuple): A tuple representing the dimensions of the array.\n",
        "\n",
        "        Returns:\n",
        "            A nested list (multi-dimensional gergen) filled with zeros.\n",
        "        \"\"\"\n",
        "        if not shape:  # If shape is empty or reaches the end of recursion\n",
        "            return 0\n",
        "        # Recursively build nested lists\n",
        "        return [gergen.custom_zeros(shape[1:]) for _ in range(shape[0])]\n",
        "\n",
        "    # HELPER\n",
        "    @staticmethod\n",
        "    def prod(iterable):\n",
        "        \"\"\"\n",
        "        Utility function to calculate the product of elements in an iterable.\n",
        "        \"\"\"\n",
        "        result = 1\n",
        "        for i in iterable:\n",
        "            result *= i\n",
        "        return result\n",
        "\n",
        "    def __mul__(self, other: Union['gergen', int, float]) -> 'gergen':\n",
        "        mul_operation = Mul()\n",
        "        result_gergen = mul_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __rmul__(self, other: Union['gergen', int, float]) -> 'gergen':\n",
        "        mul_operation = Mul()\n",
        "        result_gergen = mul_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __truediv__(self, other: Union['gergen', int, float]) -> 'gergen':\n",
        "        div_operation = TrueDiv()\n",
        "        result_gergen = div_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __rtruediv__(self, other: Union['gergen', int, float]) -> 'gergen':\n",
        "        div_operation = TrueDiv()\n",
        "        result_gergen = div_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __add__(self, other):\n",
        "        add_operation = Add()\n",
        "        result_gergen = add_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __radd__(self, other):\n",
        "        add_operation = Add()\n",
        "        result_gergen = add_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __sub__(self, other):\n",
        "        sub_operation = Sub()\n",
        "        result_gergen = sub_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def __rsub__(self, other):\n",
        "        sub_operation = Sub()\n",
        "        result_gergen = sub_operation(other, self)\n",
        "        return result_gergen\n",
        "\n",
        "    def uzunluk(self):\n",
        "        # Returns the total number of elements in the gergen\n",
        "        total = 1\n",
        "        for ele in self.__boyut:\n",
        "            total *= ele\n",
        "        return total\n",
        "\n",
        "    def boyut(self):\n",
        "        # Returns the shape of the gergen\n",
        "        return self.__boyut\n",
        "\n",
        "    def devrik(self):\n",
        "        \"\"\"\n",
        "        Returns the transpose of the gergen object.\n",
        "\n",
        "        Returns:\n",
        "            gergen: The transpose of the gergen object.\n",
        "        \"\"\"\n",
        "        if self.uzunluk() == 1:\n",
        "            return gergen(self.__veri)\n",
        "        # Check if the gergen object represents a 1D list (vector)\n",
        "        if isinstance(self.__veri, list) and all(not isinstance(item, list) for item in self.__veri):\n",
        "            # Convert each element into a list (column vector)\n",
        "            return gergen([[item] for item in self.__veri])\n",
        "        else:\n",
        "            # Handle higher-dimensional cases (e.g., 2D matrices, 3D tensors, etc.)\n",
        "            new_boyut = tuple(reversed(self.__boyut))\n",
        "            order = list(reversed(range(len(self.__boyut))))\n",
        "            arr = self.custom_zeros(\n",
        "                new_boyut)  # Assuming custom_zeros initializes an array with the given shape\n",
        "            paths = [0] * len(self.__boyut)\n",
        "            while paths[0] < self.__boyut[0]:\n",
        "                ref = self.listeye()\n",
        "                place = arr\n",
        "                for i in range(len(paths) - 1):\n",
        "                    ref = ref[paths[i]]\n",
        "                    place = place[paths[order[i]]]\n",
        "\n",
        "                place[paths[order[-1]]] = ref[paths[-1]]\n",
        "                paths[-1] += 1\n",
        "                for i in range(len(paths) - 1, 0, -1):\n",
        "                    if paths[i] >= self.__boyut[i]:\n",
        "                        paths[i] = 0\n",
        "                        paths[i - 1] += 1\n",
        "                    else:\n",
        "                        break\n",
        "            self.D = gergen(arr)\n",
        "            return gergen(arr)\n",
        "\n",
        "    def L1(self):\n",
        "        # Calculates and returns the L1 norm\n",
        "        flattened_data = self.duzlestir().__veri  # Assuming flatten returns a gergen object\n",
        "        # Calculate the L1 norm by summing the absolute values of elements in the flattened list\n",
        "        l1_norm = sum(abs(item) for item in flattened_data)\n",
        "        return l1_norm\n",
        "\n",
        "    def L2(self):\n",
        "        # Assuming flatten returns a gergen object and __veri holds the flattened data\n",
        "        flattened_data = self.duzlestir().__veri\n",
        "        # Calculate the L2 norm by summing the squares of elements in the flattened list and then taking the square root\n",
        "        l2_norm = sum(item**2 for item in flattened_data)**0.5\n",
        "        return l2_norm\n",
        "\n",
        "    def Lp(self, p):\n",
        "        # Calculates and returns the Lp norm, where p should be positive integer\n",
        "        if p <= 0:\n",
        "            raise ValueError(\"p must be a positive integer for Lp norm.\")\n",
        "        # Assuming flatten returns a gergen object and __veri holds the flattened data\n",
        "        flattened_data = self.duzlestir().__veri\n",
        "\n",
        "        # Calculate the Lp norm by raising elements to the power of p, summing, and then taking the p-th root\n",
        "        lp_norm = sum(abs(item)**p for item in flattened_data)**(1 / p)\n",
        "\n",
        "        return lp_norm\n",
        "\n",
        "    def listeye(self):\n",
        "        # Converts the gergen object into a list or a nested list, depending on its dimensions.\n",
        "        if isinstance(self.__veri, list):\n",
        "            if not self.__veri:\n",
        "                return []\n",
        "            return self.__veri.copy()\n",
        "        else:\n",
        "            return self.__veri\n",
        "\n",
        "    def duzlestir(self):\n",
        "        \"\"\"\n",
        "        Flattens a multidimensional list (self.__veri) into a 1D list.\n",
        "\n",
        "        Returns:\n",
        "            gergen: A new gergen object with the flattened list.\n",
        "        \"\"\"\n",
        "        if not isinstance(self.__veri, list):\n",
        "            return gergen(self.__veri)\n",
        "        flattened_list = []\n",
        "        # Create a stack with the initial list\n",
        "        stack = [self.__veri]\n",
        "\n",
        "        # Process the stack\n",
        "        while stack:\n",
        "            current_item = stack.pop()\n",
        "            if isinstance(current_item, list):\n",
        "                # Extend the stack by reversing the current item list\n",
        "                # to maintain the original order in the flattened list\n",
        "                stack.extend(current_item[::-1])\n",
        "            else:\n",
        "                # If it's not a list, add it to the flattened list\n",
        "                flattened_list.append(current_item)\n",
        "\n",
        "        # Since we're appending elements to the end, but processing the stack in LIFO order,\n",
        "        # we need to reverse the flattened list to restore the original element order\n",
        "        flattened_list.reverse()\n",
        "\n",
        "        # Create a new gergen instance with the flattened list\n",
        "        return gergen(flattened_list)\n",
        "\n",
        "    def boyutlandir(self, yeni_boyut):\n",
        "        \"\"\"\n",
        "        Reshapes the gergen object to a new shape 'yeni_boyut', specified as a tuple.\n",
        "        \"\"\"\n",
        "        # Flatten the data first\n",
        "        flat_data = list(self.duzlestir().__veri)\n",
        "\n",
        "        def reshape_helper(data, dims):\n",
        "            if not dims:\n",
        "                return data.pop(0)\n",
        "            return [reshape_helper(data, dims[1:]) for _ in range(dims[0])]\n",
        "\n",
        "        # Check if the new shape is compatible with the number of elements\n",
        "        if self.prod(yeni_boyut) != len(flat_data):\n",
        "            raise ValueError(\"New shape must have the same number of elements as the original.\")\n",
        "\n",
        "        # Use the helper to create the reshaped data and update the object's internal state\n",
        "        self.__veri = reshape_helper(flat_data, yeni_boyut)\n",
        "        self.__boyut = yeni_boyut\n",
        "\n",
        "    def ic_carpim(self, other):\n",
        "        ic_carpim_operation = IcCarpim()\n",
        "        result_gergen = ic_carpim_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def dis_carpim(self, other):\n",
        "        dis_carpim_operation = DisCarpim()\n",
        "        result_gergen = dis_carpim_operation(self, other)\n",
        "        return result_gergen\n",
        "\n",
        "    def us(self, n):\n",
        "        # Applies the power function to each element of the gergen object.\n",
        "        power_operation = Us()\n",
        "        result_gergen = power_operation(self, n)\n",
        "        return result_gergen\n",
        "\n",
        "    def log(self):\n",
        "        # Applies the log function to each element of the gergen object.\n",
        "        log_operation = Log10()\n",
        "        result_gergen = log_operation(self)\n",
        "        return result_gergen\n",
        "\n",
        "    def ln(self):\n",
        "        # Applies the ln function to each element of the gergen object.\n",
        "        log_operation = Ln()\n",
        "        result_gergen = log_operation(self)\n",
        "        return result_gergen\n",
        "\n",
        "    def sin(self):\n",
        "        # Applies the sin function to each element of the gergen object.\n",
        "        sin_operation = Sin()\n",
        "        result_gergen = sin_operation(self)\n",
        "        return result_gergen\n",
        "\n",
        "    def cos(self):\n",
        "        # Applies the cos function to each element of the gergen object.\n",
        "        cos_operation = Cos()\n",
        "        result_gergen = cos_operation(self)\n",
        "        return result_gergen\n",
        "\n",
        "    def tan(self):\n",
        "        # Applies the tan function to each element of the gergen object.\n",
        "        tan_operation = Tan()\n",
        "        result_gergen = tan_operation(self)\n",
        "        return result_gergen\n",
        "\n",
        "    def topla(self, eksen=None):\n",
        "        # Calculates the sum of the elements of the gergen object, optionally along a specified axis 'eksen'.\n",
        "        topla_operation = Topla()\n",
        "        result_gergen = topla_operation(self, eksen=eksen)\n",
        "        return result_gergen\n",
        "\n",
        "    def ortalama(self, eksen=None):\n",
        "        # Calculates the average of the elements of the gergen object, optionally along a specified axis 'eksen'.\n",
        "        ortalama_operation = Ortalama()\n",
        "        result = ortalama_operation(self, eksen=eksen)\n",
        "        return result\n",
        "\n",
        "    def turev_al(self, grad_output=1):\n",
        "        \"\"\"\n",
        "        Computes the backward pass of the operation that produced this gergen object.\n",
        "\n",
        "        Parameters:\n",
        "            grad_output: The gradient of the loss w.r.t the output of the operation.\n",
        "\n",
        "        Returns:\n",
        "            None: Gradients are propagated recursively.\n",
        "        \"\"\"\n",
        "        self.turev = grad_output\n",
        "\n",
        "        if self.operation == None:\n",
        "            return grad_output\n",
        "        elif type(self.operation) == Add or type(self.operation) == IcCarpim:\n",
        "            g1, g2 = self.operation.geri(self.turev)\n",
        "            self.operation.operands[0].turev_al(g1)\n",
        "            self.operation.operands[1].turev_al(g2)\n",
        "        elif type(self.operation) == ReLU or type(self.operation) == Softmax:\n",
        "            g1 = self.operation.geri(self.turev)\n",
        "            self.operation.x.turev_al(g1)\n",
        "        elif len(self.operation.operands) == 1:\n",
        "            # Typically it should not be entering here, maybe for division\n",
        "            self.operation.operands[0].turev_al(grad_output)\n",
        "        elif len(self.operation.operands) == 2:\n",
        "            # Typically it should not be entering here\n",
        "            self.operation.operands[0].turev_al(grad_output)\n",
        "            self.operation.operands[1].turev_al(grad_output)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Operation not implemented yet\" + str(self.operation))\n",
        "        self.operation = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ9O1fcCrgeL"
      },
      "source": [
        "# 2 The MLP Implementation\n",
        "\n",
        "Now, you need to complete the MLP implementation. Your task is to complete the MLP implementation by following the steps outlined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pmKGAAcrnZ8"
      },
      "source": [
        "## 2.1 Katman Class\n",
        "\n",
        "To complete MLP implementation, we first need to implement Katman (Layer) class. Implementing the Katman class involves defining its structure and operational methods such as the necessary mathematical operations, integrating activation functions, and setting up mechanisms for learning the layer's parameters during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bi7tAIYrMecf"
      },
      "outputs": [],
      "source": [
        "class Katman:\n",
        "\n",
        "    def __init__(self, input_size, output_size, activation=None):\n",
        "        \"\"\"\n",
        "        Initializes the layer with given input size, output size, and optional activation function.\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.activation = activation\n",
        "\n",
        "        # Using He initialization if activation is 'relu', otherwise Xavier\n",
        "        stddev = math.sqrt(2. / input_size) if activation == ReLU else math.sqrt(1. / input_size)\n",
        "        self.weights = rastgele_gercek((output_size, input_size)) * stddev\n",
        "        self.biases = rastgele_gercek((output_size, 1))\n",
        "        # print(\"weights: \\n\", self.weights)\n",
        "        # print(\"biases: \\n\", self.biases)\n",
        "\n",
        "    def ileri(self, x):\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the layer using matrix multiplication followed by adding biases.\n",
        "\n",
        "        Parameters:\n",
        "            x (gergen): Input to the layer.\n",
        "        \n",
        "        Returns:\n",
        "            gergen: Output of the layer.\n",
        "        \"\"\"\n",
        "        # Create an instance of IcCarpim operation\n",
        "        matrix_multiplication = IcCarpim()\n",
        "\n",
        "        # Compute the matrix multiplication of input x and weights\n",
        "        z = matrix_multiplication.ileri(self.weights, x)\n",
        "        z = z + self.biases\n",
        "\n",
        "        # Apply activation function if specified\n",
        "        if self.activation == ReLU:\n",
        "            relu_op = ReLU()\n",
        "            z = relu_op.ileri(z)\n",
        "        elif self.activation == Softmax:\n",
        "            softmax_op = Softmax()\n",
        "            z = softmax_op.ileri(z, dim=1)\n",
        "\n",
        "        return z\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"Layer with input size {self.input_size}, output size {self.output_size}, \" \\\n",
        "               f\"activation {self.activation if self.activation else 'None'}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA9RkEM2SbAl"
      },
      "source": [
        "## 2.2 ReLU Operation\n",
        "\n",
        "The `ReLU` class encapsulates the Rectified Linear Unit (ReLU) activation function. Characterized by the formula $f(x) = \\max(0, x)$, the ReLU function modifies the input tensor by setting all its negative elements to zero while preserving the positive values.\n",
        "\n",
        "The implementation of the `ReLU` class needs two principal methods:\n",
        "\n",
        "1. **`ileri(self, x)`:** Termed `ileri` to denote the forward propagation phase, this method applies the ReLU function on an input tensor `x`.\n",
        "\n",
        "2. **`geri(self, grad_input)`:** Labeled `geri`, indicating the backward propagation stage, this function is tasked with calculating the gradient of the ReLU function relative to the input tensor, given a gradient input `grad_input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9hmUxk2snVDl"
      },
      "outputs": [],
      "source": [
        "class ReLU(Operation):\n",
        "\n",
        "    def ileri(self, x):\n",
        "        \"\"\"\n",
        "        Perform the ReLU activation function on the input.\n",
        "\n",
        "        Parameters:\n",
        "            x (gergen): Input to the ReLU function.\n",
        "\n",
        "        Returns:\n",
        "            gergen: Output of the ReLU function.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        result_list = apply_elementwise(x, lambda val: max(0, val))\n",
        "        return gergen(result_list, operation=self, requires_grad=True)\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the ReLU function.\n",
        "        Gradient is passed to only those inputs where the input was greater than zero.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input (gergen): Gradient of the output of the ReLU function.\n",
        "\n",
        "        Returns:\n",
        "            gergen: Gradient of the ReLU function.\n",
        "        \"\"\"\n",
        "        grad = apply_elementwise(self.x, lambda val: 1 if val > 0 else 0)\n",
        "        result_gergen = gergen(grad)\n",
        "        return grad_input * result_gergen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bxPgnTtTVct"
      },
      "source": [
        "## 2.3 Softmax Operation\n",
        "\n",
        "The `Softmax` class is designed to implement the Softmax activation function. The Softmax function converts the raw output scores from the model into probabilities by taking the exponential of each output and then normalizing these values by dividing by the sum of all the exponentials. This results in an output vector where each component represents the probability of the corresponding class, and the sum of all components is 1.\n",
        "\n",
        "Implementing the `Softmax` class involves defining two key methods:\n",
        "\n",
        "1. **`ileri(self, x)`:** This method, named `ileri` for the forward pass, applies the Softmax function to an input tensor `x`.\n",
        "\n",
        "2. **`geri(self, grad_input)`:** The `geri` method, indicating the backward pass, is responsible for computing the gradient of the Softmax function with respect to the input tensor, given an input gradient `grad_input`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KxyRQS9OvUTo"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "class Softmax(Operation):\n",
        "\n",
        "    def ileri(self, x, dim=1):\n",
        "        \"\"\"\n",
        "        Apply the Softmax activation function to the input.\n",
        "\n",
        "        Parameters:\n",
        "            x (gergen): Input to the Softmax function.\n",
        "\n",
        "        Returns:\n",
        "            gergen: Output of the Softmax function.\n",
        "        \"\"\"\n",
        "        self.x = x\n",
        "        self.dim = dim\n",
        "\n",
        "        # Compute the softmax of the input x\n",
        "        result = []\n",
        "        data = x.veri if dim == 0 else x.devrik().veri  # Transpose if dim is 0\n",
        "\n",
        "        for row in data:\n",
        "            exps = [math.exp(val) for val in row]\n",
        "            sum_exps = sum(exps)\n",
        "            softmax_vals = [exp_val / sum_exps for exp_val in exps]\n",
        "            result.append(softmax_vals)\n",
        "\n",
        "        result = result if dim == 0 else gergen(result).devrik().veri  # Transpose back if dim is 0\n",
        "        return gergen(result, operation=self, requires_grad=True)\n",
        "\n",
        "    def geri(self, grad_input):\n",
        "        \"\"\"\n",
        "        Compute the gradient of the Softmax function.\n",
        "\n",
        "        Parameters:\n",
        "            grad_input (gergen): Gradient of the output of the Softmax function. ?????\n",
        "\n",
        "        Returns:\n",
        "            gergen: Gradient of the Softmax function.\n",
        "        \"\"\"\n",
        "        return grad_input # Since we are taking this derivative with cross entropy loss inside training loop\n",
        "    \n",
        "        # softmax_output = self.ileri(self.x, self.dim).veri\n",
        "        # result = []\n",
        "\n",
        "        # # Compute the Jacobian matrix for each row in the softmax output\n",
        "        # for outputs in softmax_output:\n",
        "        #     jacobian_matrix = [\n",
        "        #         [s_i * (int(i == j) - s_j) for j, s_j in enumerate(outputs)] for i, s_i in enumerate(outputs)\n",
        "        #     ]\n",
        "        #     result.append(jacobian_matrix)\n",
        "\n",
        "        # # Transpose back if dim isn't 1 to match the input's original shape\n",
        "        # if self.dim != 1:\n",
        "        #     result = [list(row) for row in zip(*result)]\n",
        "\n",
        "        # return gergen(result[0]) * grad_input\n",
        "\n",
        "# print(\"-------SOFTMAX-------\")\n",
        "\n",
        "# print(\"---ileri---\")\n",
        "\n",
        "# # gergen softmax\n",
        "# print(\"gergen softmax\")\n",
        "# g1 = gergen([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).devrik()\n",
        "# softmax = Softmax()\n",
        "# res = softmax.ileri(g1, dim=1)\n",
        "# print(\"result of gergen softmax: \\n\", res)\n",
        "\n",
        "# print(\"#############################################\")\n",
        "\n",
        "# # torch softmax\n",
        "# print(\"torch softmax\")\n",
        "# t1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
        "# t_softmax = torch.nn.Softmax(dim=1)\n",
        "# t_res = t_softmax(t1)\n",
        "# print(\"result of torch softmax: \\n\", t_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFKNHLC9H2WR"
      },
      "source": [
        "## 2.4 MLP Class\n",
        "\n",
        "The `MLP` class is a template for creating our custom MLP.\n",
        "\n",
        "When setting up (`__init__`), you need to define:\n",
        "- `input_size`: The shape of input layer.\n",
        "- `hidden_size`: The shape of the hidden layer.\n",
        "- `output_size`: How many outputs you need at the end, like how many categories you're classifying.\n",
        "\n",
        "The main job of this setup is to prepare the layers with their settings and connections.\n",
        "\n",
        "The `ileri` method takes your data (`x`) and sends it through all the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "kDFRKfctMhKu"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the MLP with input, hidden, and output layers\n",
        "\n",
        "        Parameters:\n",
        "            input_size (int): Number of input features\n",
        "            hidden_size (int): Number of hidden units\n",
        "            output_size (int): Number of output units\n",
        "        \n",
        "        Returns:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.hidden_layer = Katman(input_size, hidden_size, activation=ReLU)\n",
        "        self.output_layer = Katman(hidden_size, output_size, activation=Softmax)\n",
        "\n",
        "    def ileri(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP\n",
        "\n",
        "        Parameters:\n",
        "            x (gergen): Input to the MLP\n",
        "        \n",
        "        Returns:\n",
        "            gergen: Output of the MLP\n",
        "        \"\"\"\n",
        "        hidden_output = self.hidden_layer.ileri(x)\n",
        "        output = self.output_layer.ileri(hidden_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBF0KFquVFAd"
      },
      "source": [
        "## 2.5 Cross-Entropy Loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sTR6BXBNQ90C"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "def cross_entropy(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    The cross-entropy loss function for multi-class classification.\n",
        "    Remember, in a multi-class classification context, y_true is typically represented in a one-hot encoded format.\n",
        "    \n",
        "    Parameters:\n",
        "        y_pred (gergen): Predicted probabilities for each class in each sample\n",
        "        y_true (gergen): True labels.\n",
        "\n",
        "    Returns:\n",
        "        float : The cross-entropy loss\n",
        "    \"\"\"\n",
        "    epsilon = 1e-12  # Small value to avoid log(0)\n",
        "    n = y_pred.boyut()[0]\n",
        "\n",
        "    loss = (y_true * (y_pred + epsilon).ln()).topla()\n",
        "    loss = loss / -n # Average loss over all samples to obtain smaller losses\n",
        "    return loss.veri\n",
        "\n",
        "# g3 = gergen([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).devrik()\n",
        "# g2 = gergen([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]).devrik()\n",
        "# softmax = Softmax()\n",
        "# res = softmax.ileri(g3, dim=1)\n",
        "# print(\"result of gergen softmax: \\n\", res)\n",
        "# print(\"cross entropy loss: \", cross_entropy(res, g2))\n",
        "\n",
        "# # torch softmax\n",
        "# t1 = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=torch.float32)\n",
        "# t2 = torch.tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0], dtype=torch.float32)\n",
        "# t_softmax = torch.nn.Softmax(dim=0)\n",
        "# t_res = t_softmax(t1)\n",
        "# print(\"result of torch softmax: \\n\", t_res)\n",
        "# t_loss = nn.CrossEntropyLoss()\n",
        "# t_loss_res = t_loss(t1, t2)\n",
        "# print(\"cross entropy loss: \", t_loss_res.item())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zk_5AQtW-Ma"
      },
      "source": [
        "## 2.6 Implementing the training pipeline with `egit()`\n",
        "\n",
        "The `egit()` function adjusts the model's weights and biases to decrease errors and improve predictions through epochs. Here's a simplified overview of its components and steps:\n",
        "\n",
        "### Main Components:\n",
        "\n",
        "- **`mlp`**: The MLP model that we implemented.\n",
        "- **`inputs`**: The data fed into the model.\n",
        "- **`targets`**: The labels for each input\n",
        "- **`epochs`**: The number of complete passes through the training dataset.\n",
        "- **`learning_rate`**: How much the model's weights are adjusted during training to minimize error.\n",
        "\n",
        "You need to implement these training steps:\n",
        "\n",
        "1. **Forward Pass**\n",
        "\n",
        "2. **Calculate Loss**\n",
        "\n",
        "3. **Backward Pass**\n",
        "\n",
        "4. **Update Parameters**\n",
        "\n",
        "5. **Reset Gradients**\n",
        "\n",
        "6. **Loss Reporting**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "svEUrEmMMxLO"
      },
      "outputs": [],
      "source": [
        "def egit(mlp, inputs, targets, epochs, learning_rate):\n",
        "    \"\"\"\n",
        "    Trains the provided MLP model using the input data and targets.\n",
        "\n",
        "    Parameters:\n",
        "        mlp (MLP): The MLP model with an `ileri` method for forward propagation.\n",
        "        inputs (list or generator): Input data to train on (each sample should be formatted as a gergen object).\n",
        "        targets (list or generator): Expected outputs (each target should be formatted as a gergen object).\n",
        "        epochs (int): Number of epochs to run.\n",
        "        learning_rate (float): Step size for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "        list: The loss values after each epoch to visualize the learning progress.\n",
        "    \"\"\"\n",
        "    # To track loss values\n",
        "    loss_curve = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        # print(\"target size: \", targets.boyut())\n",
        "        # print(\"input size: \", inputs.boyut()) # 20.000, MNIST small dataset\n",
        "        for i in range(len(inputs)):\n",
        "            # Convert the input and target to `gergen` objects\n",
        "            x_gergen = inputs[i]\n",
        "            y_gergen = targets[i]\n",
        "            # print(\"x_gergen type: \", type(x_gergen))\n",
        "            # print(\"y_gergen type: \", type(y_gergen))\n",
        "\n",
        "            x_gergen = gergen(x_gergen).devrik()\n",
        "            y_gergen = gergen(y_gergen).devrik()\n",
        "\n",
        "            # Normalize the input data to prevent overflow\n",
        "            x_gergen = x_gergen / 255\n",
        "\n",
        "            # Forward pass: predict with the MLP\n",
        "            predictions = mlp.ileri(x_gergen)\n",
        "\n",
        "            # Compute the loss via cross-entropy\n",
        "            loss = cross_entropy(predictions, y_gergen)\n",
        "\n",
        "            grad = predictions - y_gergen\n",
        "            grad = grad / predictions.boyut()[0]\n",
        "            # print(\"grad: \", grad)\n",
        "\n",
        "            # Backward pass: calculate gradients using `turev_al`\n",
        "            predictions.turev_al(grad)\n",
        "\n",
        "            # Update parameters of hidden and output layers\n",
        "            mlp.hidden_layer.weights = mlp.hidden_layer.weights - learning_rate * mlp.hidden_layer.weights.turev\n",
        "            mlp.hidden_layer.biases = mlp.hidden_layer.biases - learning_rate * mlp.hidden_layer.biases.turev\n",
        "            # mlp.hidden_layer.weights.turev = None\n",
        "            # mlp.hidden_layer.biases.turev = None\n",
        "            mlp.hidden_layer.weights.requires_grad = False\n",
        "            mlp.hidden_layer.biases.requires_grad = False\n",
        "            mlp.hidden_layer.weights.operation = None\n",
        "            mlp.hidden_layer.biases.operation = None\n",
        "\n",
        "            mlp.output_layer.weights = mlp.output_layer.weights - learning_rate * mlp.output_layer.weights.turev\n",
        "            mlp.output_layer.biases = mlp.output_layer.biases - learning_rate * mlp.output_layer.biases.turev\n",
        "            # mlp.output_layer.weights.turev = None\n",
        "            # mlp.output_layer.biases.turev = None\n",
        "            mlp.output_layer.weights.requires_grad = False\n",
        "            mlp.output_layer.biases.requires_grad = False\n",
        "            mlp.output_layer.weights.operation = None\n",
        "            mlp.output_layer.biases.operation = None\n",
        "\n",
        "            # Accumulate the loss for this batch\n",
        "            total_loss += loss\n",
        "\n",
        "        # Append the average loss for this epoch\n",
        "        average_loss = total_loss / len(inputs)\n",
        "        loss_curve.append(average_loss)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}: Loss = {average_loss}\")\n",
        "\n",
        "    return loss_curve\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYj01h9FYRSR"
      },
      "source": [
        "## 2.7 Implementing the testing pipeline with `test()`\n",
        "\n",
        "The `test()` measures the trained model's performance in test data.\n",
        "\n",
        "### Main Components:\n",
        "\n",
        "- **`mlp`**: The model that we trained with egit().\n",
        "- **`inputs`**: Testing data.\n",
        "- **`targets`**: Labels for testing data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LzW05UXlVP2X"
      },
      "outputs": [],
      "source": [
        "def test(mlp, inputs, targets):\n",
        "    \"\"\"\n",
        "    Tests the MLP model with the input data and computes the loss.\n",
        "\n",
        "    Parameters:\n",
        "        mlp (MLP): The MLP model with an `ileri` method for forward propagation.\n",
        "        inputs (list or generator): Input data to test (each sample should be formatted as a gergen object).\n",
        "        targets (list or generator): Expected outputs (each target should be formatted as a gergen object).\n",
        "\n",
        "    Returns:\n",
        "        float: The computed average loss over the test dataset.\n",
        "        float: The accuracy of the model on the test dataset.\n",
        "    \"\"\"\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "\n",
        "    for i in range(len(inputs)):\n",
        "        # Same operations as in the training loop without the backward pass\n",
        "        x_gergen = gergen(inputs[i]).devrik()\n",
        "        y_gergen = gergen(targets[i]).devrik()\n",
        "\n",
        "        # Normalize the input data\n",
        "        x_gergen = x_gergen / 255\n",
        "\n",
        "        # Forward pass: predict with the MLP\n",
        "        predictions = mlp.ileri(x_gergen)\n",
        "\n",
        "        # Compute the loss via cross-entropy\n",
        "        loss = cross_entropy(predictions, y_gergen)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Determine the predicted label by choosing the class with the highest probability\n",
        "        predicted_index = predictions.devrik().veri[0].index(max(predictions.devrik().veri[0]))\n",
        "        predicted_class = predicted_index\n",
        "        actual_index = y_gergen.devrik().veri[0].index(max(y_gergen.devrik().veri[0]))\n",
        "        actual_class = actual_index\n",
        "        \n",
        "        # print(\"predicted_index: \", predicted_index)\n",
        "        # print(\"actual_index: \", actual_index)\n",
        "\n",
        "        if predicted_class == actual_class:\n",
        "            correct += 1\n",
        "\n",
        "    # Calculate average loss and accuracy\n",
        "    average_loss = total_loss / len(inputs)\n",
        "    accuracy = correct / len(inputs)\n",
        "\n",
        "    print(f\"Test Loss: {average_loss}\")\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.3f}%\")\n",
        "\n",
        "    return average_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-4U07WQZNSE"
      },
      "source": [
        "## 2.9 Training and Testing our custom MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10: Loss = 0.10425540496646732\n",
            "Epoch 2/10: Loss = 0.04302860007117107\n",
            "Epoch 3/10: Loss = 0.03643918786997279\n",
            "Epoch 4/10: Loss = 0.03353215227519106\n",
            "Epoch 5/10: Loss = 0.03165693429550655\n",
            "Epoch 6/10: Loss = 0.030224698241965022\n",
            "Epoch 7/10: Loss = 0.028961830934225788\n",
            "Epoch 8/10: Loss = 0.02783940279441188\n",
            "Epoch 9/10: Loss = 0.026872357900386175\n",
            "Epoch 10/10: Loss = 0.026038014745106197\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train_data_path = \"train_data.csv\"\n",
        "test_data_path = \"test_data.csv\"\n",
        "data, labels = data_preprocessing(train_data_path)\n",
        "test_data, test_labels = data_preprocessing(test_data_path)\n",
        "\n",
        "# convert data to list\n",
        "\n",
        "data = data.values.tolist()\n",
        "labels = labels.tolist()\n",
        "test_data = test_data.values.tolist()\n",
        "test_labels = test_labels.tolist()\n",
        "\n",
        "# print(\"type data: \", type(data))\n",
        "# print(\"type labels: \", type(labels))\n",
        "\n",
        "# Initialize the MLP with input, hidden, and output layers\n",
        "input_size = 28 * 28\n",
        "hidden_size = 10\n",
        "output_size = 10\n",
        "mlp = MLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Train the MLP using your preferred training loop\n",
        "epochs = 10\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Egit\n",
        "loss_list = egit(mlp, data, labels, epochs, learning_rate)\n",
        "\n",
        "# print(\"size of test data: \", len(test_data))\n",
        "# Test\n",
        "# test_loss = test(mlp, test_data, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.02759809985415833\n",
            "Test Accuracy: 91.979%\n"
          ]
        }
      ],
      "source": [
        "# Complete test\n",
        "test_loss, accuracy = test(mlp, test_data, test_labels)\n",
        "\n",
        "# To test with specific inputs\n",
        "# sample_test = test_data[3142]\n",
        "# sample_label = test_labels[3142]\n",
        "\n",
        "# sample_test = gergen(sample_test).devrik()\n",
        "# sample_label = gergen(sample_label).devrik()\n",
        "\n",
        "# sample_test = sample_test / 255\n",
        "\n",
        "# sample_prediction = mlp.ileri(sample_test)\n",
        "\n",
        "# loss = cross_entropy(sample_prediction, sample_label)\n",
        "# print(\"Sample Test Loss: \", loss)\n",
        "\n",
        "# print(\"Sample Test Prediction Veri: \", sample_prediction.veri)\n",
        "# print(\"Sample Test Prediction devrik Veri: \", sample_prediction.devrik().veri[0])\n",
        "# print(\"Sample Test Prediction devrik Veri Max: \", max(sample_prediction.devrik().veri[0]))\n",
        "# print(\"Sample Test Prediction devrik Veri Max Index: \", sample_prediction.devrik().veri[0].index(max(sample_prediction.devrik().veri[0])))\n",
        "\n",
        "# predicted_index = sample_prediction.devrik().veri[0].index(max(sample_prediction.devrik().veri[0]))\n",
        "# predicted_class = predicted_index\n",
        "\n",
        "# print(\"Actual Label Veri: \", sample_label.veri)\n",
        "# print(\"Actual Label devrik Veri: \", sample_label.devrik().veri[0])\n",
        "# print(\"Actual Label devrik Veri Max: \", max(sample_label.devrik().veri[0]))\n",
        "# print(\"Actual Label devrik Veri Max Index: \", sample_label.devrik().veri[0].index(max(sample_label.devrik().veri[0])))\n",
        "\n",
        "# actual_index = sample_label.devrik().veri[0].index(max(sample_label.devrik().veri[0]))\n",
        "# actual_class = actual_index\n",
        "\n",
        "# print(\"Predicted Class: \", predicted_class)\n",
        "# print(\"Actual Class: \", actual_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8oQAeqgY8y9"
      },
      "source": [
        "## 2.8 Data Handling Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "he4K9vfbKp4f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def data_preprocessing(data_file):\n",
        "    \"\"\"\n",
        "    Preprocess the dataset provided in CSV format.\n",
        "    \n",
        "    Args:\n",
        "        data_file (str): Path to the CSV file containing the dataset.\n",
        "        \n",
        "    Returns:\n",
        "        data (pd.DataFrame): Feature data\n",
        "        labels (np.ndarray): One-hot encoded labels\n",
        "    \"\"\"\n",
        "    # Load the data into a Pandas DataFrame\n",
        "    df = pd.read_csv(data_file)\n",
        "    \n",
        "    # Extract the first column as labels\n",
        "    labels = df.iloc[:, 0]\n",
        "    \n",
        "    # One-hot encode the labels using LabelBinarizer\n",
        "    lb = LabelBinarizer()\n",
        "    one_hot_labels = lb.fit_transform(labels)\n",
        "    \n",
        "    # Extract the remaining columns as feature data\n",
        "    data = df.iloc[:, 1:]\n",
        "    \n",
        "    return data, one_hot_labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iULhajyWtIy6"
      },
      "source": [
        "## 2.10 The Loss Curve\n",
        "\n",
        "One of the first things we should do when analyzing a model is to plot the loss curve. We should ideally see a smoothly decreasing curve over iterations/epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "bsNPqhALtidx"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_loss_curve(loss_list):\n",
        "    \"\"\"\n",
        "    Plots the loss curve given a list of loss values.\n",
        "    \n",
        "    Parameters:\n",
        "        loss_list (list of floats): The list containing the loss values after each epoch.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, len(loss_list) + 1), loss_list, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(\"Training Loss Curve\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xticks(range(1, len(loss_list) + 1))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the loss curve\n",
        "plot_loss_curve(loss_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn0_ApnFpR-b"
      },
      "source": [
        "## 2.11 Find the Best Hyperparameters\n",
        "\n",
        "Let us train the model for different values for our hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "aXArza5apaTI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.05715960390068871\n",
            "Test Accuracy: 82.038%\n",
            "learning rate=0.01 and hidden layer size=5 provided train_loss=0.056 and test_loss=0.057\n",
            "Test Loss: 0.041712625485869986\n",
            "Test Accuracy: 88.079%\n",
            "learning rate=0.01 and hidden layer size=10 provided train_loss=0.040 and test_loss=0.042\n",
            "Test Loss: 0.03951758744754205\n",
            "Test Accuracy: 88.569%\n",
            "learning rate=0.01 and hidden layer size=30 provided train_loss=0.037 and test_loss=0.040\n",
            "Test Loss: 0.19850720763034368\n",
            "Test Accuracy: 43.404%\n",
            "learning rate=0.001 and hidden layer size=5 provided train_loss=0.203 and test_loss=0.199\n",
            "Test Loss: 0.18793229310244738\n",
            "Test Accuracy: 51.345%\n",
            "learning rate=0.001 and hidden layer size=10 provided train_loss=0.193 and test_loss=0.188\n",
            "Test Loss: 0.16985782298196628\n",
            "Test Accuracy: 67.617%\n",
            "learning rate=0.001 and hidden layer size=30 provided train_loss=0.176 and test_loss=0.170\n",
            "Test Loss: 0.23048341668753108\n",
            "Test Accuracy: 12.131%\n",
            "learning rate=0.0001 and hidden layer size=5 provided train_loss=0.231 and test_loss=0.230\n",
            "Test Loss: 0.22540589342103265\n",
            "Test Accuracy: 11.831%\n",
            "learning rate=0.0001 and hidden layer size=10 provided train_loss=0.226 and test_loss=0.225\n",
            "Test Loss: 0.22859291791683242\n",
            "Test Accuracy: 13.891%\n",
            "learning rate=0.0001 and hidden layer size=30 provided train_loss=0.229 and test_loss=0.229\n",
            "Test Loss: 0.25930071617722167\n",
            "Test Accuracy: 10.091%\n",
            "learning rate=1e-05 and hidden layer size=5 provided train_loss=0.257 and test_loss=0.259\n",
            "Test Loss: 0.23772267369700087\n",
            "Test Accuracy: 5.471%\n",
            "learning rate=1e-05 and hidden layer size=10 provided train_loss=0.238 and test_loss=0.238\n",
            "Test Loss: 0.2494768111757116\n",
            "Test Accuracy: 17.392%\n",
            "learning rate=1e-05 and hidden layer size=30 provided train_loss=0.251 and test_loss=0.249\n",
            "\n",
            "Lowest test loss achieved: <__main__.MLP object at 0x3067bea50> with params hl=30 and lr=0.01\n"
          ]
        }
      ],
      "source": [
        "learning_rates = [1e-2, 1e-3, 1e-4, 1e-5]\n",
        "hidden_layer_sizes = [5,10,30]\n",
        "epochs = 10\n",
        "\n",
        "# results is dictionary mapping tuples of the form\n",
        "# (learning_rate, hidden_layer_size) to tuples of the form\n",
        "# (training_loss, test_loss).\n",
        "results = {}\n",
        "best_loss = 10   # The lowest test loss that we have seen so far.\n",
        "best_model = None # The MLP object that achieved the lowest test loss.\n",
        "best_loss_list = None  # The loss list for the best model\n",
        "best_lr = None  # The learning rate for the best model\n",
        "best_hl = None  # The hidden layer size for the best model\n",
        "\n",
        "train_data, train_labels = data_preprocessing(\"train_data.csv\")\n",
        "test_data, test_labels = data_preprocessing(\"test_data.csv\")\n",
        "\n",
        "data = train_data.values.tolist()\n",
        "labels = train_labels.tolist()\n",
        "test_data = test_data.values.tolist()\n",
        "test_labels = test_labels.tolist()\n",
        "\n",
        "for lr in learning_rates:\n",
        "\n",
        "    for hl in hidden_layer_sizes:\n",
        "\n",
        "        # Create and train a new MLP instance\n",
        "        mlp = MLP(input_size=input_size, hidden_size=hl, output_size=output_size)\n",
        "        loss_list = egit(mlp, data, labels, epochs, lr)\n",
        "        train_loss = loss_list[-1]\n",
        "\n",
        "        # Predict values for test set and calculate test loss\n",
        "        test_loss, test_accuracy = test(mlp, test_data, test_labels)\n",
        "\n",
        "        print(f\"learning rate={lr} and hidden layer size={hl} provided train_loss={train_loss:.3f} and test_loss={test_loss:.3f}\")\n",
        "\n",
        "        # Save the results\n",
        "        results[(lr,hl)] = (train_loss, test_loss)\n",
        "        if test_loss < best_loss:\n",
        "            best_lr = lr\n",
        "            best_hl = hl\n",
        "            best_loss = test_loss\n",
        "            best_model = mlp\n",
        "            best_loss_list = loss_list\n",
        "\n",
        "# best_model = mlp\n",
        "# test_loss, test_accuracy = test(mlp, test_data, test_labels)\n",
        "\n",
        "print(f'\\nLowest test loss achieved: {best_model} with params hl={best_hl} and lr={best_lr}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-bUqdLUt21f"
      },
      "source": [
        "## 2.12 Plot the Loss Curve of the Best Model\n",
        "\n",
        "Let us analyze some aspects of the best model. To keep things short, let us just plot the loss history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hRq1YsiruCDL"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsQAAAIjCAYAAAAEFA25AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1TUlEQVR4nO3dd3gU9drG8e+mJ/SaQgtFqVIEjCgYkS4qCChNKSI2omBEEVSqCigiFoQDCnKkCgewHAUC0pReYkFA4IDUUEQINQnJvH/Mm5UlCQkh2dlk7s91zZXJ7OzM8yQQbia/+Y3DMAwDERERERGb8rK6ABERERERKykQi4iIiIitKRCLiIiIiK0pEIuIiIiIrSkQi4iIiIitKRCLiIiIiK0pEIuIiIiIrSkQi4iIiIitKRCLiIiIiK0pEItkolevXoSHh2frvcOHD8fhcORsQZJnLFmyhLp16xIQEIDD4eDMmTNWl2Rr4eHh9OrVK1vvdTgcDB8+PEv7btq0CT8/P/7880/ntnvvvZd77703W+e2C4fDQVRUVKb7ff755zgcDg4cOJD7RWXDnXfeySuvvGJ1GXKDFIglz3I4HFlaVq1aZXWplujVqxcFCxa0uowsW7RoEW3atKFkyZL4+fkRFhbGo48+yg8//GB1adny119/8eijjxIYGMjEiRP54osvKFCgQK6dLzUkXL2ULl2apk2b8v333+faeS9evMjw4cOz/Pds1apVzvpmzpyZ7j533303DoeDWrVq5WCl7vPaa6/RtWtXKlSo4PZzJyQkMGjQIMLCwggMDCQiIoKYmJgsv//IkSM8+uijFC1alMKFC9OuXTv+97//pdlv0qRJPPLII5QvXx6Hw5Ht/2hY7e233+bOO++kVKlSBAQEcMsttzBgwABOnjyZZt+UlBTeeecdKlasSEBAALVr12bOnDlp9hs0aBATJ04kLi7OHS1IDvGxugCR7Priiy9cPv/3v/9NTExMmu3Vq1e/qfNMnTqVlJSUbL339ddf59VXX72p8+d3hmHwxBNP8Pnnn1OvXj2io6MJCQnh2LFjLFq0iGbNmvHTTz9x1113WV3qDdm8eTPnzp1j1KhRNG/e3G3nHTlyJBUrVsQwDI4fP87nn3/O/fffzzfffMMDDzyQ4+e7ePEiI0aMALihK6ABAQHMnj2bxx57zGX7gQMHWLduHQEBATlZptvExsayfPly1q1bZ8n5e/XqxYIFCxgwYAC33HKL8/u/cuVKGjdufN33nj9/nqZNm3L27FmGDBmCr68v77//PpGRkcTGxlKiRAnnvmPHjuXcuXPccccdHDt2LLfbyjVbt26lbt26dOnShUKFCrFz506mTp3Kf//7X2JjY13+E/vaa68xZswY+vbtS8OGDfnqq6/o1q0bDoeDLl26OPdr164dhQsX5pNPPmHkyJFWtCXZYYjkE/369TOy8kf6woULbqjGej179jQKFChgdRmZevfddw3AGDBggJGSkpLm9X//+9/Gxo0bb/o8KSkpxsWLF2/6OFk1Y8YMAzA2b96cY8c8f/58hq9Nnz493fOdPn3a8PX1Nbp165ZjdVzt5MmTBmAMGzYsS/uvXLnSAIwOHToYPj4+xsmTJ11ef+utt4zg4GCjcePGRs2aNXO01goVKhg9e/bM1nuz2uMLL7xglC9fPs2f5cjISCMyMvK677106ZKRnJycrfoMwzA2btxoAMa7777rcszKlSsbjRo1yvT9Y8eONQBj06ZNzm07d+40vL29jcGDB7vse+DAAWePBQoUyPbX9WqA0a9fv0z3S/2zvn///ps+Z3oWLFhgAMacOXOc2w4fPmz4+vq61JeSkmI0adLEKFu2rHHlyhWXY0RFRRkVKlRI92eaeCYNmZB87d5776VWrVps3bqVe+65h6CgIIYMGQLAV199Rdu2bQkLC8Pf35/KlSszatQokpOTXY5x7RjiAwcO4HA4GDduHFOmTKFy5cr4+/vTsGFDNm/e7PLe9MYQp46TW7x4MbVq1cLf35+aNWuyZMmSNPWvWrWKBg0aEBAQQOXKlfnXv/6V4+OS58+fT/369QkMDKRkyZI89thjHDlyxGWfuLg4evfuTdmyZfH39yc0NJR27dq5jOHbsmULrVq1omTJkgQGBlKxYkWeeOKJ65770qVLjB49mmrVqjFu3Lh0+3r88ce54447gIzHZKc3pjA8PJwHHniApUuX0qBBAwIDA/nXv/5FrVq1aNq0aZpjpKSkUKZMGTp16uSybcKECdSsWZOAgACCg4N5+umn+fvvv6/b17333kvPnj0BaNiwYZpfKWfla5465GXfvn3cf//9FCpUiO7du1/3vOkpWrQogYGB+Pi4/kIwq71d7/t64MABSpUqBcCIESOcQyGyMta2Xbt2+Pv7M3/+fJfts2fP5tFHH8Xb2zvNe65cucKoUaOcf+fCw8MZMmQICQkJLvsZhsGbb75J2bJlCQoKomnTpuzYsSPdOs6cOcOAAQMoV64c/v7+VKlShbFjx2b7t0KLFy/mvvvuy/TvaOrQkblz5/L6669TpkwZgoKCiI+Pz9Z5ARYsWIC3tzdPPfWUc1tAQAB9+vRh/fr1HDp0KNP3N2zYkIYNGzq3VatWjWbNmvHll1+67FuhQoVcuz8iKz8bc1Pqz/urx/x/9dVXJCUl8dxzzzm3ORwOnn32WQ4fPsz69etdjtGiRQv+/PNPYmNj3VCx5AQNmZB876+//qJNmzZ06dKFxx57jODgYMAMUQULFiQ6OpqCBQvyww8/MHToUOLj43n33XczPe7s2bM5d+4cTz/9NA6Hg3feeYcOHTrwv//9D19f3+u+98cff2ThwoU899xzFCpUiA8//JCOHTty8OBB568lt2/fTuvWrQkNDWXEiBEkJyczcuRIZwDJCZ9//jm9e/emYcOGjB49muPHj/PBBx/w008/sX37dooWLQpAx44d2bFjB88//zzh4eGcOHGCmJgYDh486Py8ZcuWlCpVildffZWiRYty4MABFi5cmOnX4fTp0wwYMCDdAHSzdu/eTdeuXXn66afp27cvVatWpXPnzgwfPpy4uDhCQkJcajl69KjLrz6ffvpp59fohRdeYP/+/Xz88cds376dn376KcPv82uvvUbVqlWZMmWKcwhD5cqVgax/zcEMgK1ataJx48aMGzeOoKCgTHs+e/Ysp06dwjAMTpw4wUcffcT58+fTDE3ISm+ZfV9LlSrFpEmTePbZZ3n44Yfp0KEDALVr1860zqCgINq1a8ecOXN49tlnAfj555/ZsWMHn376Kb/88kua9zz55JPMmDGDTp068dJLL7Fx40ZGjx7Nzp07WbRokXO/oUOH8uabb3L//fdz//33s23bNlq2bEliYqLL8S5evEhkZCRHjhzh6aefpnz58qxbt47Bgwdz7NgxJkyYkGkfVzty5AgHDx7k9ttvz/J7Ro0ahZ+fHwMHDiQhIQE/Pz9SUlI4ffp0lt5fpEgR55/D7du3c+utt1K4cGGXfVL/QxkbG0u5cuXSPU5KSgq//PJLuv+JveOOO1i2bBnnzp2jUKFCWe4tO7LyszE958+f5/Lly5ke39fXlyJFirhsMwyDv/76iytXrrBnzx5effVVvL29XYYAbd++nQIFCqQZgpf6td2+fbvLkJT69esD8NNPP1GvXr1M6xIPYPEVapEck96QicjISAMwJk+enGb/9H59/vTTTxtBQUHG5cuXndt69uxpVKhQwfn5/v37DcAoUaKEcfr0aef2r776ygCMb775xrlt2LBhaWoCDD8/P2Pv3r3ObT///LMBGB999JFz24MPPmgEBQUZR44ccW7bs2eP4ePjk6WhIZkNmUhMTDRKly5t1KpVy7h06ZJz+7fffmsAxtChQw3DMIy///47za9hr7Vo0aJsDQ/44IMPDMBYtGhRlvZP7+tpGOn/CrVChQoGYCxZssRl3927d6f5WhuGYTz33HNGwYIFnX8u1q5dawDGrFmzXPZbsmRJutszqunqr0lWv+aGYX7/AOPVV1+97nmuPd+1i7+/v/H555+77JvV3rLyfc3ukIn58+cb3377reFwOIyDBw8ahmEYL7/8slGpUiXDMMy/u1cPmYiNjTUA48knn3Q53sCBAw3A+OGHHwzDMIwTJ04Yfn5+Rtu2bV1+XT1kyBADcPnV/qhRo4wCBQoYf/zxh8sxX331VcPb29tZl2FkbcjE8uXL0/wMSHXtkInUr0OlSpXS/CxK/RmTlWXlypXO99WsWdO477770px7x44dGf4cTJX6fRw5cmSa1yZOnGgAxq5du9J9b04OmcjKz8b0/r6n/n3JbElv2MqxY8dc9ilbtqwxb948l33atm3r/LN5tQsXLmT499TPz8949tlns/GVECtoyITke/7+/vTu3TvN9sDAQOf6uXPnOHXqFE2aNOHixYvs2rUr0+N27tyZYsWKOT9v0qQJQLp3ZF+refPmziuGYF5RK1y4sPO9ycnJLF++nPbt2xMWFubcr0qVKrRp0ybT42fFli1bOHHiBM8995zLDUxt27alWrVq/Pe//wXMr5Ofnx+rVq3KcKhA6lXNb7/9lqSkpCzXkPrr4dy66lSxYkVatWrlsu3WW2+lbt26zJs3z7ktOTmZBQsW8OCDDzr/XMyfP58iRYrQokULTp065Vzq169PwYIFWbly5Q3Xk9Wv+dVSr55m1cSJE4mJiSEmJoaZM2fStGlTnnzySZer9VntLbvf16xq2bIlxYsXZ+7cuRiGwdy5c+natWu6+3733XcAREdHu2x/6aWXAJxfu+XLl5OYmMjzzz/v8iv9AQMGpDnm/PnzadKkCcWKFXP5OjRv3pzk5GTWrFlzQ/389ddfAC4/FzLTs2dPl59FACEhIc7vYWZLnTp1nO+7dOkS/v7+ac6R+mft0qVLGdaR+lp2359TMvvZmJFXXnklS1+v9957L817ixcvTkxMDN988w0jR46kZMmSnD9/3mWf7HxtU/9cSd6gIROS75UpUwY/P78023fs2MHrr7/ODz/8kGbc3tmzZzM9bvny5V0+T/1HMLPxpem9N/X9qe89ceIEly5dokqVKmn2S29bdqTOkVq1atU0r1WrVo0ff/wRMP+BHDt2LC+99BLBwcHceeedPPDAA/To0cM55CAyMpKOHTsyYsQI3n//fe69917at29Pt27d0v1HJFXqr3bPnTuXIz1dq2LFiulu79y5M0OGDOHIkSOUKVOGVatWceLECTp37uzcZ8+ePZw9e5bSpUune4wTJ07ccD1Z/Zqn8vHxoWzZsjd0jjvuuIMGDRo4P+/atSv16tUjKiqKBx54AD8/vyz3lt3va1b5+vryyCOPMHv2bO644w4OHTpEt27d0t33zz//xMvLK82f/5CQEIoWLer82qZ+vOWWW1z2K1WqVJqgumfPHn755ZcMhyFl53sM5q/gsyq9P6MBAQHZmpkkMDAwzXhqwDmU4Nrgfe17gWy/P6dk9rMxIzVq1KBGjRrZOqefn5/z6/3AAw/QrFkz7r77bkqXLu2cmSU7X1vDMDQPfR6iQCz5Xno/qM6cOUNkZCSFCxdm5MiRVK5cmYCAALZt28agQYOydENNRmNes/KP4c281woDBgzgwQcfZPHixSxdupQ33niD0aNH88MPP1CvXj0cDgcLFixgw4YNfPPNNyxdupQnnniC9957jw0bNmQ4H3K1atUA+PXXX2nfvn2mdWT0j8u1N0Kmyugf8M6dOzN48GDmz5/PgAED+PLLLylSpAitW7d27pOSkkLp0qWZNWtWusfIybHcGfH398fL6+Z+kefl5UXTpk354IMP2LNnDzVr1sxyb9n9vt6Ibt26MXnyZIYPH06dOnUyDTU5GTBSUlJo0aJFhg9RuPXWW2/oeKljXLPyn+JU6f0ZTU5OTnce3PQUL17c+R/+0NDQNDdnAs5p0a7+bVN6x/H39093CrWsvD+nZPdn49mzZ7N0BdvPz4/ixYtfd5+77rqL0NBQZs2a5QzEoaGhrFy5Mk3Ivd7X5syZM5QsWTLTmsQzKBCLLa1atYq//vqLhQsXcs899zi379+/38Kq/lG6dGkCAgLYu3dvmtfS25YdqQ8N2L17N/fdd5/La7t3707zUIHKlSvz0ksv8dJLL7Fnzx7q1q3Le++95/JwhTvvvJM777yTt956i9mzZ9O9e3fmzp3Lk08+mW4NjRs3plixYsyZM4chQ4ZkemNd6hW+M2fOuNx8dvUTwbKiYsWK3HHHHcybN4+oqCgWLlxI+/btXa56Vq5cmeXLl3P33Xfn2JWxG/2a55QrV64AOH8NfKO9Xe/7erMBtXHjxpQvX55Vq1YxduzYDPerUKECKSkp7Nmzx+XGpuPHj3PmzBnn1y714549e6hUqZJzv5MnT6YJqpUrV+b8+fM5Nk906n/wbvbnyKFDhzL87ca1Vq5c6bz5q27duqxcuZL4+HiXG+s2btzofD0jXl5e3HbbbWzZsiXNaxs3bqRSpUq5fkPdzejfvz8zZszIdL/IyMgsPUTm8uXLLr8prFu3Lp9++ik7d+50+U9bRl/bI0eOkJiYeNPz4Iv7aAyx2FJq8Lr6qkNiYiKffPKJVSW58Pb2pnnz5ixevJijR486t+/duzfHnjrWoEEDSpcuzeTJk11+Ffj999+zc+dO2rZtC5h34l9793blypUpVKiQ831///13mis4qf9ApPdrxlRBQUEMGjSInTt3MmjQoHSvAs2cOZNNmzY5zwu4jO28cOFClv4hvFbnzp3ZsGED06ZN49SpUy7DJQAeffRRkpOTGTVqVJr3XrlyJVuPYc7q1zwnJSUlsWzZMvz8/Jz/OGe1t6x8X1NnvsjuY6kdDgcffvghw4YN4/HHH89wv/vvvx8gzcwP48ePB3B+7Zo3b46vry8fffSRS+3pzRjx6KOPsn79epYuXZrmtTNnzjj/I5FVZcqUoVy5cumGyhuR3THEnTp1Ijk5mSlTpji3JSQkMH36dCIiIlxmmDh48GCaeyU6derE5s2bXerfvXs3P/zwA4888shN9ZTbsjOG+MKFC1y8eDHNsf7zn//w999/uww9ateuHb6+vi7/RhiGweTJkylTpkyaBwdt3boVIM89UMjOdIVYbOmuu+6iWLFi9OzZkxdeeAGHw8EXX3zhUUMWhg8fzrJly7j77rt59tlnSU5O5uOPP6ZWrVpZntsyKSmJN998M8324sWL89xzzzF27Fh69+5NZGQkXbt2dU4BFh4ezosvvgjAH3/8QbNmzXj00UepUaMGPj4+LFq0iOPHjzunKJsxYwaffPIJDz/8MJUrV+bcuXNMnTqVwoULO4NMRl5++WV27NjBe++9x8qVK+nUqRMhISHExcWxePFiNm3a5HzqV8uWLSlfvjx9+vTh5Zdfxtvbm2nTplGqVCkOHjx4A19dMwwNHDiQgQMHUrx48TRXCSMjI3n66acZPXo0sbGxtGzZEl9fX/bs2cP8+fP54IMPXOYszgpfX98sfc1vxvfff+8MOidOnGD27NnOqaRSrxpmtbesfF8DAwOpUaMG8+bN49Zbb6V48eLUqlXrhh673K5dO9q1a3fdferUqUPPnj2ZMmWKc8jTpk2bmDFjBu3bt3fOLV2qVCkGDhzI6NGjeeCBB7j//vvZvn0733//fZpfX7/88st8/fXXPPDAA/Tq1Yv69etz4cIFfv31VxYsWMCBAwdu+Ffe7dq1Y9GiRTc1fjS7Y4gjIiJ45JFHGDx4MCdOnKBKlSrMmDGDAwcO8Nlnn7ns26NHD1avXu3yM++5555j6tSptG3bloEDB+Lr68v48eMJDg523ryY6ptvvuHnn38GzJ8zv/zyi/NnzUMPPeSceu/AgQNUrFiRnj178vnnn99wT1mVnTHEe/bsoXnz5nTu3Jlq1arh5eXFli1bmDlzJuHh4fTv39+5b9myZRkwYADvvvsuSUlJNGzYkMWLF7N27VpmzZqV5rdbMTExlC9fXlOu5SXun9hCJHdkNO1aRk+7+umnn4w777zTCAwMNMLCwoxXXnnFWLp0aZqpjDKadi29aci4ZmqmjKZdS+9pTOk9RWvFihVGvXr1DD8/P6Ny5crGp59+arz00ktGQEBABl+Ff1xvGqLKlSs795s3b55Rr149w9/f3yhevLjRvXt34/Dhw87XT506ZfTr18+oVq2aUaBAAaNIkSJGRESE8eWXXzr32bZtm9G1a1ejfPnyhr+/v1G6dGnjgQceMLZs2ZJpnakWLFhgtGzZ0ihevLjh4+NjhIaGGp07dzZWrVrlst/WrVuNiIgIw8/Pzyhfvrwxfvz4DKdda9u27XXPeffdd6c7ldfVpkyZYtSvX98IDAw0ChUqZNx2223GK6+8Yhw9evS6x87oyXGGkfnX3DBu/EmD6U27FhAQYNStW9eYNGlSuk/Myqy3rH5f161bZ9SvX9/w8/PLdHqyq6ddu570/u4mJSUZI0aMMCpWrGj4+voa5cqVMwYPHuwyTaJhGEZycrIxYsQIIzQ01AgMDDTuvfde47fffkv379i5c+eMwYMHG1WqVDH8/PyMkiVLGnfddZcxbtw4IzEx0blfZn2l2rZtmwEYa9euTdNPetOuZfZ1uFGXLl0yBg4caISEhBj+/v5Gw4YN00w9mFpPehHg0KFDRqdOnYzChQsbBQsWNB544AFjz549afa73s+X6dOnO/f79ddfszx9YFZ/NubUk+pOnjxpPPXUU86fbX5+fsYtt9xiDBgwIM0TFA3D/HP19ttvGxUqVDD8/PyMmjVrGjNnzkx3v9DQUOP111+/qfrEvRyG4UGXxEQkU+3bt2fHjh3s2bPH6lJEJB3NmjUjLCyML774wupSLPfJJ5/wyiuvsG/fPudDkfK7xYsX061bN/bt20doaKjV5UgWaQyxiAe79q7pPXv28N1337k8QUlEPMvbb7/NvHnzbvhmz/xo5cqVvPDCC7YJwwBjx44lKipKYTiP0RViEQ8WGhpKr169qFSpEn/++SeTJk0iISGB7du3p5lnVURERLJHN9WJeLDWrVszZ84c4uLi8Pf3p1GjRrz99tsKwyIiIjlIV4hFRERExNY0hlhEREREbE2BWERERERsTWOIsyklJYWjR49SqFChm350qYiIiIjkPMMwOHfuHGFhYXh5ZXwdWIE4m44ePeryGEwRERER8UyHDh2ibNmyGb6uQJxNhQoVAswvcOrjUHNTUlISy5Ytcz5i1U7s2rtd+wb79m7XvkG927F3u/YN6t2dvcfHx1OuXDlnbsuIAnE2pQ6TKFy4sNsCcVBQEIULF7blXx479m7XvsG+vdu1b1Dvduzdrn2Derei98yGt+qmOhERERGxNQViEREREbE1BWIRERERsTUFYhERERGxNQViEREREbE1BWIRERERsTUFYhERERGxNQViEREREbE1BWIRERERsTUFYhERERGxNQViEREREbE1BWIRERERsTUFYhERERGxNQXiPCA5GVavdrBmTRlWr3aQnGx1RSIiIiL5hwKxh1u4EMLDoUULH8aPb0CLFj6Eh5vbRUREROTmKRB7sIULoVMnOHzYdfuRI+Z2hWIRERGRm6dA7KGSk6F/fzCMtK+lbhswAA2fEBEREblJCsQeau3atFeGr2YYcOiQuZ+IiIiIZJ8CsYc6dixn9xMRERGR9CkQe6jQ0JzdT0RERETSp0DsoZo0gbJlweFI/3WHA8qVM/cTERERkexTIPZQ3t7wwQfmekaheMIEcz8RERERyT4FYg/WoQMsWABlyrhu9/U1t3foYE1dIiIiIvmJArGH69ABDhyAmJgrPPNMLF5eBklJUL261ZWJiIiI5A8KxHmAtzdERhq0bv0nrVubkxDPnGlxUSIiIiL5hAJxHtOtWwpgBuKUFIuLEREREckHFIjzmAcfNChcGA4e1EM5RERERHKCAnEeExgInTqZ6xo2ISIiInLzFIjzoMceMz/Onw+XL1tbi4iIiEhep0CcB0VGmg/lOHsWvv3W6mpERERE8jYF4jzIywu6dzfXv/jC2lpERERE8joF4jwqddjEd9/BqVPW1iIiIiKSlykQ51E1a0K9enDlCnz5pdXViIiIiORdCsR5WOpVYg2bEBEREck+BeI8rGtXczzxhg2wZ4/V1YiIiIjkTQrEeVhoKLRoYa7PmmVtLSIiIiJ5lQJxHpc6bGLmTDAMa2sRERERyYsUiPO4hx+GAgVg3z5z6ISIiIiI3BgF4jyuQAHo0MFc1811IiIiIjdOgTgfSB02MW8eJCZaW4uIiIhIXqNAnA80a2beYHf6NHz/vdXViIiIiOQtCsT5gLc3dOtmrmvYhIiIiMiNUSDOJ1KHTXzzDfz9t7W1iIiIiOQlCsT5RJ06UKuWOYZ4wQKrqxERERHJOxSI8wmHQ49yFhEREckOBeJ8pHt3MxivXQsHDlhdjYiIiEjeoECcj5QtC02bmut6lLOIiIhI1igQ5zNXD5vQo5xFREREMqdAnM907AgBAbB7N2zdanU1IiIiIp5PgTifKVwY2rc313VznYiIiEjmFIjzodRhE3PmQFKStbWIiIiIeDqPCMQTJ04kPDycgIAAIiIi2LRpU4b7Tp06lSZNmlCsWDGKFStG8+bN0+xvGAZDhw4lNDSUwMBAmjdvzp49e1z2OX36NN27d6dw4cIULVqUPn36cP78+Vzpz91atoRSpeDkSYiJsboaEREREc9meSCeN28e0dHRDBs2jG3btlGnTh1atWrFiRMn0t1/1apVdO3alZUrV7J+/XrKlStHy5YtOXLkiHOfd955hw8//JDJkyezceNGChQoQKtWrbh8+bJzn+7du7Njxw5iYmL49ttvWbNmDU899VSu9+sOvr7QpYu5rmETIiIiItdneSAeP348ffv2pXfv3tSoUYPJkycTFBTEtGnT0t1/1qxZPPfcc9StW5dq1arx6aefkpKSwooVKwDz6vCECRN4/fXXadeuHbVr1+bf//43R48eZfHixQDs3LmTJUuW8OmnnxIREUHjxo356KOPmDt3LkePHnVX67nq8cfNj4sXQ3y8paWIiIiIeDQfK0+emJjI1q1bGTx4sHObl5cXzZs3Z/369Vk6xsWLF0lKSqJ48eIA7N+/n7i4OJo3b+7cp0iRIkRERLB+/Xq6dOnC+vXrKVq0KA0aNHDu07x5c7y8vNi4cSMPP/xwmvMkJCSQkJDg/Dz+/1NmUlISSW4YqJt6jqyeq04duPVWH/74w8H8+Vfo0SPvzsF2o73nF3btG+zbu137BvV+9Ue7sGvfoN6v/uiu82XG0kB86tQpkpOTCQ4OdtkeHBzMrl27snSMQYMGERYW5gzAcXFxzmNce8zU1+Li4ihdurTL6z4+PhQvXty5z7VGjx7NiBEj0mxftmwZQUFBWao1J8TcwKDgBg1u5Y8/qvPBB39TsuS6XKzKPW6k9/zErn2DfXu3a9+g3u3Irn2DeneHixcvZmk/SwPxzRozZgxz585l1apVBAQE5Oq5Bg8eTHR0tPPz+Ph45/jlwoUL5+q5wfwfTkxMDC1atMDX1zdL76lRA2bPht9+K0nt2vdTtmwuF5lLstN7fmDXvsG+vdu1b1Dvduzdrn2Dendn7/FZHDdqaSAuWbIk3t7eHD9+3GX78ePHCQkJue57x40bx5gxY1i+fDm1a9d2bk993/HjxwkNDXU5Zt26dZ37XHvT3pUrVzh9+nSG5/X398ff3z/Ndl9fX7f+Yb6R891yCzRpAmvXOpg/35dXXsnl4nKZu7/WnsKufYN9e7dr36De7di7XfsG9e6O3rN6DktvqvPz86N+/frOG+IA5w1yjRo1yvB977zzDqNGjWLJkiUu44ABKlasSEhIiMsx4+Pj2bhxo/OYjRo14syZM2y96lFuP/zwAykpKURERORUex5Bj3IWERERuT7LZ5mIjo5m6tSpzJgxg507d/Lss89y4cIFevfuDUCPHj1cbrobO3Ysb7zxBtOmTSM8PJy4uDji4uKccwg7HA4GDBjAm2++yddff82vv/5Kjx49CAsLo/3/P8KtevXqtG7dmr59+7Jp0yZ++uknoqKi6NKlC2FhYW7/GuSmRx4BPz/47Tf45RerqxERERHxPJaPIe7cuTMnT55k6NChxMXFUbduXZYsWeK8Ke7gwYN4ef2T2ydNmkRiYiKdOnVyOc6wYcMYPnw4AK+88goXLlzgqaee4syZMzRu3JglS5a4jDOeNWsWUVFRNGvWDC8vLzp27MiHH36Y+w27WbFi8MADsHCheZW4Th2rKxIRERHxLJYHYoCoqCiioqLSfW3VqlUunx84cCDT4zkcDkaOHMnIkSMz3Kd48eLMnj37RsrMsx5/3AzEs2fD2LHg7W11RSIiIiKew/IhE5L77r8fiheHY8fghx+srkZERETEsygQ24CfHzz6qLmuRzmLiIiIuFIgtonURzkvXAgXLlhbi4iIiIgnUSC2iUaNoFIlMwwvXmx1NSIiIiKeQ4HYJhwO1zmJRURERMSkQGwjqYE4Jgbi4qytRURERMRTKBDbyC23QEQEpKTAnDlWVyMiIiLiGRSIbSb15joNmxARERExKRDbTOfO4OMD27fDjh1WVyMiIiJiPQVimylZEtq0MddnzrS2FhERERFPoEBsQ6nDJmbNMscTi4iIiNiZArENPfggFC4Mhw7BmjVWVyMiIiJiLQViGwoIgEceMdd1c52IiIjYnQKxTaUOm1iwAC5dsrYWERERESspENtUkyZQvjzEx8M331hdjYiIiIh1FIhtyssLunc31zVsQkREROxMgdjGUh/lvGQJnDxpbS0iIiIiVlEgtrEaNeD22+HKFZg3z+pqRERERKyhQGxzepSziIiI2J0Csc117Qre3rBpE/zxh9XViIiIiLifArHNBQdDixbmuh7lLCIiInakQCzOYRMzZ4JhWFuLiIiIiLspEAvt20PBgrB/P6xbZ3U1IiIiIu6lQCwEBUGHDua6bq4TERERu1EgFuCfYRNffgkJCdbWIiIiIuJOCsQCQNOmEBYGf/8N331ndTUiIiIi7qNALIA59Vq3bua6hk2IiIiInSgQi1PqsIn//hdOn7a2FhERERF3USAWp9q14bbbIDER5s+3uhoRERER91AgFhdXz0ksIiIiYgcKxOKiWzdwOODHH815iUVERETyOwVicVGmDNx3n7muq8QiIiJiBwrEkoYe5SwiIiJ2okAsaXToAIGB8McfsHmz1dWIiIiI5C4FYkmjUCFo395c15zEIiIikt8pEEu6UodNzJ0LSUnW1iIiIiKSmxSIJV0tWkDp0nDqFCxdanU1IiIiIrlHgVjS5eMDXbua65ptQkRERPIzBWLJUOqwia++grNnra1FREREJLcoEEuGbr8dqlWDy5fhP/+xuhoRERGR3KFALBlyOPQoZxEREcn/FIjlurp1Mz+uWgWHDllaioiIiEiuUCCW6woPh3vuMZ9YN2uW1dWIiIiI5DwFYslU6rCJL77Qo5xFREQk/1Eglkx16gT+/vD77xAba3U1IiIiIjlLgVgyVbQoPPigua6b60RERCS/USCWLEkdNjF7Nly5Ym0tIiIiIjlJgViypHVrKFEC4uJgxQqrqxERERHJOZYH4okTJxIeHk5AQAARERFs2rQpw3137NhBx44dCQ8Px+FwMGHChDT7pL527dKvXz/nPvfee2+a15955pncaC/f8PODzp3NdQ2bEBERkfzE0kA8b948oqOjGTZsGNu2baNOnTq0atWKEydOpLv/xYsXqVSpEmPGjCEkJCTdfTZv3syxY8ecS0xMDACPPPKIy359+/Z12e+dd97J2ebyocceMz8uXAjnz1tbi4iIiEhOsTQQjx8/nr59+9K7d29q1KjB5MmTCQoKYtq0aenu37BhQ9599126dOmCv79/uvuUKlWKkJAQ5/Ltt99SuXJlIiMjXfYLCgpy2a9w4cI53l9+c+edUKUKXLwIixZZXY2IiIhIzvCx6sSJiYls3bqVwYMHO7d5eXnRvHlz1q9fn2PnmDlzJtHR0TgcDpfXZs2axcyZMwkJCeHBBx/kjTfeICgoKMNjJSQkkJCQ4Pw8Pj4egKSkJJKSknKk3utJPYc7znU9Xbt6MWqUN198kUKXLsluOaen9O5udu0b7Nu7XfsG9X71R7uwa9+g3q/+6K7zZcayQHzq1CmSk5MJDg522R4cHMyuXbty5ByLFy/mzJkz9OrVy2V7t27dqFChAmFhYfzyyy8MGjSI3bt3s3DhwgyPNXr0aEaMGJFm+7Jly64bpHNa6hAQq4SFFQCas2KFg5kzV1C8eEKm78kpVvduFbv2Dfbt3a59g3q3I7v2DerdHS5evJil/SwLxO7w2Wef0aZNG8LCwly2P/XUU8712267jdDQUJo1a8a+ffuoXLlyuscaPHgw0dHRzs/j4+MpV64cLVu2dMtwi6SkJGJiYmjRogW+vr65fr7rmTEjhQ0bvDh1qgWPPZaS6+fzpN7dya59g317t2vfoN7t2Ltd+wb17s7eU3+jnxnLAnHJkiXx9vbm+PHjLtuPHz+e4Q1zN+LPP/9k+fLl173qmyoiIgKAvXv3ZhiI/f390x237Ovr69Y/zO4+X3p69IANG2D2bG9eftnbbef1hN6tYNe+wb6927VvUO927N2ufYN6d0fvWT2HZTfV+fn5Ub9+fVZcNaltSkoKK1asoFGjRjd9/OnTp1O6dGnatm2b6b6x//884tDQ0Js+rx08+ij4+pqPcf7tN6urEREREbk5ls4yER0dzdSpU5kxYwY7d+7k2Wef5cKFC/Tu3RuAHj16uNx0l5iYSGxsLLGxsSQmJnLkyBFiY2PZu3evy3FTUlKYPn06PXv2xMfH9SL4vn37GDVqFFu3buXAgQN8/fXX9OjRg3vuuYfatWvnftP5QIkScP/95rrmJBYREZG8ztIxxJ07d+bkyZMMHTqUuLg46taty5IlS5w32h08eBAvr38y+9GjR6lXr57z83HjxjFu3DgiIyNZtWqVc/vy5cs5ePAgTzzxRJpz+vn5sXz5ciZMmMCFCxcoV64cHTt25PXXX8+9RvOhxx6Dr76CWbPg7bfBy/JHvIiIiIhkj+U31UVFRREVFZXua1eHXDCfQmcYRqbHbNmyZYb7lStXjtWrV99wneLqgQegSBE4fBhWrYL77rO6IhEREZHs0XU9yZaAAHMsMWjYhIiIiORtCsSSbamPcl6wwHx6nYiIiEhepEAs2da4MVSoAOfOwddfW12NiIiISPYoEEu2eXn9c5VYwyZEREQkr1IglpuSGoiXLIETJ6ytRURERCQ7FIjlplSrBg0aQHIyzJtndTUiIiIiN06BWG5a6lXiL76wtg4RERGR7FAglpvWtSt4e8PmzbB7t9XViIiIiNwYBWK5aaVLQ6tW5rpurhMREZG8RoFYcsTVs02kpFhbi4iIiMiNUCCWHNGuHRQqBAcOwE8/WV2NiIiISNYpEEuOCAqCjh3NdQ2bEBERkbxEgVhyTOqwiS+/hMuXra1FREREJKsUiCXH3HsvlCkDZ87Ad99ZXY2IiIhI1igQS47x9obu3c11zUksIiIieYUCseSo1GET//0v/PWXtbWIiIiIZIUCseSo226DOnUgKQnmz7e6GhEREZHMKRBLjtOjnEVERCQvUSCWHNetG3h5wbp1sG+f1dWIiIiIXJ8CseS4sDBo1sxcnzXL2lpEREREMqNALLni6mEThmFtLSIiIiLXo0AsuaJDB/PpdXv3wqZNVlcjIiIikjEFYskVBQvCww+b67q5TkRERDyZArHkmtRhE3PnQmKitbWIiIiIZESBWHJN8+YQHGw+oGPpUqurEREREUmfArHkGh8f6NrVXNewCREREfFUCsSSqx5/3Pz49ddw5oylpYiIiIikS4FYclW9elCjBiQkwH/+Y3U1IiIiImkpEEuucjj0KGcRERHxbArEkuu6dzc/rl4NBw9aW4uIiIjItRSIJdeVLw/33muu61HOIiIi4mkUiMUt9ChnERER8VQKxOIWnTqBvz/s3Anbt1tdjYiIiMg/FIjFLYoUgYceMtd1c52IiIh4EgVicZvUOYnnzIErV6ytRURERCSVArG4TevWUKIEHD8Oy5dbXY2IiIiISYFY3MbXF7p0Mdc1bEJEREQ8hQKxuFXqsIlFi+DcOWtrEREREQEFYnGzO+6AW26BS5fMUCwiIiJiNQVicSs9yllEREQ8jQKxuF1qIF6xAo4etbYWEREREQVicbtKleCuu8wn1s2ebXU1IiIiYncKxGKJ1JvrNGxCRERErKZALJZ49FFzGrZffjEXEREREasoEIsliheHtm3N9Zkzra1FRERE7E2BWCyTOmxi1ixITra2FhEREbEvBWKxTNu2ULSoOdPEqlVWVyMiIiJ2pUAslvH3N8cSg26uExEREetYHognTpxIeHg4AQEBREREsGnTpgz33bFjBx07diQ8PByHw8GECRPS7DN8+HAcDofLUq1aNZd9Ll++TL9+/ShRogQFCxakY8eOHD9+PKdbkyxIHTbxn//AxYvW1iIiIiL2ZGkgnjdvHtHR0QwbNoxt27ZRp04dWrVqxYkTJ9Ld/+LFi1SqVIkxY8YQEhKS4XFr1qzJsWPHnMuPP/7o8vqLL77IN998w/z581m9ejVHjx6lQ4cOOdqbZM1dd0F4OJw/D199ZXU1IiIiYkeWBuLx48fTt29fevfuTY0aNZg8eTJBQUFMmzYt3f0bNmzIu+++S5cuXfD398/wuD4+PoSEhDiXkiVLOl87e/Ysn332GePHj+e+++6jfv36TJ8+nXXr1rFhw4Yc71Guz8tLj3IWERERa/lYdeLExES2bt3K4MGDndu8vLxo3rw569evv6lj79mzh7CwMAICAmjUqBGjR4+mfPnyAGzdupWkpCSaN2/u3L9atWqUL1+e9evXc+edd6Z7zISEBBISEpyfx8fHA5CUlERSUtJN1ZsVqedwx7ncrXNnePNNX5YtMzh8+ArBwa6v5+fer8eufYN9e7dr36Der/5oF3btG9T71R/ddb7MWBaIT506RXJyMsHXpJ/g4GB27dqV7eNGRETw+eefU7VqVY4dO8aIESNo0qQJv/32G4UKFSIuLg4/Pz+KFi2a5rxxcXEZHnf06NGMGDEizfZly5YRFBSU7XpvVExMjNvO5U633HIPe/YUY/jwXTz44P/S3Se/9p4Zu/YN9u3drn2Dercju/YN6t0dLmbxBiXLAnFuadOmjXO9du3aREREUKFCBb788kv69OmT7eMOHjyY6Oho5+fx8fGUK1eOli1bUrhw4ZuqOSuSkpKIiYmhRYsW+Pr65vr53G3/fi9efBG2b6/JpEmuN0Hm994zYte+wb6927VvUO927N2ufYN6d2fvqb/Rz4xlgbhkyZJ4e3unmd3h+PHj171h7kYVLVqUW2+9lb179wIQEhJCYmIiZ86ccblKnNl5/f390x237Ovr69Y/zO4+n7t07w4DB8K2bV7s3etF9epp98mvvWfGrn2DfXu3a9+g3u3Yu137BvXujt6zeg7Lbqrz8/Ojfv36rFixwrktJSWFFStW0KhRoxw7z/nz59m3bx+hoaEA1K9fH19fX5fz7t69m4MHD+boeeXGlCoFrVub63qUs4iIiLiTpbNMREdHM3XqVGbMmMHOnTt59tlnuXDhAr179wagR48eLjfdJSYmEhsbS2xsLImJiRw5coTY2Fjn1V+AgQMHsnr1ag4cOMC6det4+OGH8fb2pmvXrgAUKVKEPn36EB0dzcqVK9m6dSu9e/emUaNGGd5QJ+5x9aOcU1KsrUVERETsw9IxxJ07d+bkyZMMHTqUuLg46taty5IlS5w32h08eBAvr38y+9GjR6lXr57z83HjxjFu3DgiIyNZ9f/P/j18+DBdu3blr7/+olSpUjRu3JgNGzZQqlQp5/vef/99vLy86NixIwkJCbRq1YpPPvnEPU1Lhh56CAoVgj//hB9/hHvusboiERERsQPLb6qLiooiKioq3ddSQ26q8PBwDMO47vHmzp2b6TkDAgKYOHEiEydOzHKdkvsCA6FTJ5g+3ZyTWIFYRERE3MHyRzeLXC112MT8+XD5srW1iIiIiD0oEItHiYyEsmXh7Fn49lurqxERERE7UCAWj+LlZU7BBnqUs4iIiLiHArF4nNRhE999B6dOWVuLiIiI5H8KxOJxataEunXhyhX48kurqxEREZH8ToFYPFLqVWI9pENERERymwKxeKSuXc3xxOvXw+zZDtasKcPq1Q6Sk62uTERERPIbBWLxSKGhcNtt5nqvXj6MH9+AFi18CA+HhQstLU1ERETyGQVi8UgLF8LPP6fdfuSI+fAOhWIRERHJKQrE4nGSk6F///RfS31Q4YABaPiEiIiI5AgFYvE4a9fC4cMZv24YcOiQuZ+IiIjIzVIgFo9z7FjO7iciIiJyPQrE4nFCQ3N2PxEREZHrUSAWj9OkCZQtCw5H+q87HFCunLmfiIiIyM1SIBaP4+0NH3xgrmcUiidMMPcTERERuVkKxOKROnSABQugTJm0r02dar4uIiIikhMUiMVjdegABw5ATMwVoqO3UKNGCgA7dlhbl4iIiOQvCsTi0by9ITLS4J57jvDOO2YgnjwZjh+3uDARERHJNxSIJc9o0cLgjjvg0iV47z2rqxEREZH8QoFY8gyHA4YNM9cnToSTJ62tR0RERPIHBWLJU9q0gfr14eJFGD/e6mpEREQkP1AgljzF4YChQ831jz+Gv/6yth4RERHJ+xSIJc958EGoUwfOnzfnIxYRERG5GQrEkudcfZX4ww/h77+trUdERETyNgViyZPat4fbboP4+H+eaiciIiKSHQrEkid5ecEbb5jrEybA2bOWliMiIiJ5mAKx5FkdO0KNGmYY/ugjq6sRERGRvEqBWPIsLy94/XVzffx4OHfO2npEREQkb1Igljzt0UehalXzxrqJE62uRkRERPIiBWLJ07y9/7lKPG6cORWbiIiIyI1QIJY8r0sXqFLFfEjHpElWVyMiIiJ5jQKx5Hk+PvDaa+b6u++aj3UWERERySoFYskXuneHihXh5En417+srkZERETyEgViyRd8fWHIEHP9nXfg0iVr6xEREZG8Q4FY8o0ePaBCBYiLg6lTra5GRERE8goFYsk3/Pxg8GBzfexYuHzZ2npEREQkb8hWID506BCHDx92fr5p0yYGDBjAlClTcqwwkezo1QvKloWjR2HaNKurERERkbwgW4G4W7durFy5EoC4uDhatGjBpk2beO211xg5cmSOFihyI/z94dVXzfXRoyEhwdp6RERExPNlKxD/9ttv3HHHHQB8+eWX1KpVi3Xr1jFr1iw+//zznKxP5Ib16QNhYXD4MMyYYXU1IiIi4umyFYiTkpLw9/cHYPny5Tz00EMAVKtWjWPHjuVcdSLZEBAAgwaZ62+/DYmJ1tYjIiIini1bgbhmzZpMnjyZtWvXEhMTQ+vWrQE4evQoJUqUyNECRbKjb18IDoY//4QvvrC6GhEREfFk2QrEY8eO5V//+hf33nsvXbt2pU6dOgB8/fXXzqEUIlYKDIRXXjHX33oLkpKsrUdEREQ8l0923nTvvfdy6tQp4uPjKVasmHP7U089RVBQUI4VJ3Iznn4axoyB/fth9mzo2dPqikRERMQTZesK8aVLl0hISHCG4T///JMJEyawe/duSpcunaMFimRXgQIwcKC5/tZbcOWKtfWIiIiIZ8pWIG7Xrh3//ve/AThz5gwRERG89957tG/fnkmTJuVogSI347nnoEQJ2LMH5s61uhoRERHxRNkKxNu2baNJkyYALFiwgODgYP7880/+/e9/8+GHH+ZogSI3o2BBeOklc/3NNyE52dp6RERExPNkKxBfvHiRQoUKAbBs2TI6dOiAl5cXd955J3/++WeOFihys/r1g2LFYPdumD/f6mpERETE02QrEFepUoXFixdz6NAhli5dSsuWLQE4ceIEhQsXvqFjTZw4kfDwcAICAoiIiGDTpk0Z7rtjxw46duxIeHg4DoeDCRMmpNln9OjRNGzYkEKFClG6dGnat2/P7t27Xfa59957cTgcLsszzzxzQ3VL3lG4MLz4ork+ahSkpFhbj4iIiHiWbAXioUOHMnDgQMLDw7njjjto1KgRYF4trlevXpaPM2/ePKKjoxk2bBjbtm2jTp06tGrVihMnTqS7/8WLF6lUqRJjxowhJCQk3X1Wr15Nv3792LBhAzExMSQlJdGyZUsuXLjgsl/fvn05duyYc3nnnXeyXLfkPc8/D0WKwO+/w8KFVlcjIiIiniRb06516tSJxo0bc+zYMeccxADNmjXj4YcfzvJxxo8fT9++fenduzcAkydP5r///S/Tpk3j1VdfTbN/w4YNadiwIUC6rwMsWbLE5fPPP/+c0qVLs3XrVu655x7n9qCgoAxDteQ/RYtC//4wcqS5dOgAXtn676CIiIjkN9kKxAAhISGEhIRw+PBhAMqWLXtDD+VITExk69atDB482LnNy8uL5s2bs379+uyWlcbZs2cBKF68uMv2WbNmMXPmTEJCQnjwwQd54403rjuHckJCAgkJCc7P4+PjAfMx1klueOpD6jnccS5Pk1O99+sH77/vw6+/OvjPf67Qvr2RE+XlGn3P7de7XfsG9X71R7uwa9+g3q/+6K7zZSZbgTglJYU333yT9957j/PnzwNQqFAhXnrpJV577TW8snDp7dSpUyQnJxMcHOyyPTg4mF27dmWnrHTrHDBgAHfffTe1atVybu/WrRsVKlQgLCyMX375hUGDBrF7924WXud36aNHj2bEiBFpti9btsytDyOJiYlx27k8TU703rp1NebPr8qgQefx9V2Nw5EDheUyfc/tx659g3q3I7v2DerdHS5evJil/bIViF977TU+++wzxowZw9133w3Ajz/+yPDhw7l8+TJvvfVWdg6b4/r168dvv/3Gjz/+6LL9qaeecq7fdttthIaG0qxZM/bt20flypXTPdbgwYOJjo52fh4fH0+5cuVo2bLlDd9ImB1JSUnExMTQokULfH19c/18niQne4+IgO++M9i/vyiG0Za2bT33KrG+5/br3a59g3q3Y+927RvUuzt7T/2NfmayFYhnzJjBp59+ykMPPeTcVrt2bcqUKcNzzz2XpUBcsmRJvL29OX78uMv248eP58jY3qioKL799lvWrFlD2bJlr7tvREQEAHv37s0wEPv7++Pv759mu6+vr1v/MLv7fJ4kJ3oPCYGoKBg7Ft5+24f27fH4q8T6ntuvd7v2Derdjr3btW9Q7+7oPavnyNZtRadPn6ZatWpptlerVo3Tp09n6Rh+fn7Ur1+fFStWOLelpKSwYsUK56wV2WEYBlFRUSxatIgffviBihUrZvqe2NhYAEJDQ7N9Xsk7XnoJgoJgyxb4/nurqxERERGrZSsQ16lTh48//jjN9o8//pjatWtn+TjR0dFMnTqVGTNmsHPnTp599lkuXLjgnHWiR48eLjfdJSYmEhsbS2xsLImJiRw5coTY2Fj27t3r3Kdfv37MnDmT2bNnU6hQIeLi4oiLi+PSpUsA7Nu3j1GjRrF161YOHDjA119/TY8ePbjnnntuqHbJu0qVMh/pDOaME4bnjpoQERERN8jWkIl33nmHtm3bsnz5cufV3PXr13Po0CG+++67LB+nc+fOnDx5kqFDhxIXF0fdunVZsmSJ80a7gwcPutygd/ToUZd5jseNG8e4ceOIjIxk1apVAEyaNAkwH75xtenTp9OrVy/8/PxYvnw5EyZM4MKFC5QrV46OHTvy+uuvZ+dLIXnUwIHw8cewcSPExMD/P1tGREREbChbgTgyMpI//viDiRMnOmeE6NChA0899RRvvvkmTZo0yfKxoqKiiIqKSve11JCbKjw8HCOTy3mZvV6uXDlWr16d5fokfwoOhmeegQkTYMQIaNHC88cSi4iISO7I9jzEYWFhaW6e+/nnn/nss8+YMmXKTRcmkttefhkmTYJ162DlSrjvPqsrEhERESvoWV1iW2Fh0LevuZ7OFNMiIiJiEwrEYmuDBoGfH6xZAxpJIyIiYk8KxGJrZctCnz7m+siR1tYiIiIi1rihMcQdOnS47utnzpy5mVpELPHqq/Dpp/DDD/Djj9C4sdUViYiIiDvdUCAuUqRIpq/36NHjpgoScbfy5aFXL5g6FUaNgqVLra5IRERE3OmGAvH06dNzqw4RSw0eDNOnw7JlsGED3Hmn1RWJiIiIu2gMsQhQsSKk/nJDY4lFRETsRYFY5P8NGQLe3vD997B5s9XViIiIiLsoEIv8v8qVoXt3c33UKGtrEREREfdRIBa5ypAh4OUF33wD27dbXY2IiIi4gwKxyFWqVoUuXcx1jSUWERGxBwVikWu8/jo4HLB4Mfz8s9XViIiISG5TIBa5RvXq8Oij5vqbb1pbi4iIiOQ+BWKRdLz+uvlxwQL47TdraxEREZHcpUAsko5ataBjR3P9rbesrUVERERylwKxSAbeeMP8OG8e7NxpbS0iIiKSexSIRTJQpw60bw+GoavEIiIi+ZkCsch1pF4lnjMH/vjD2lpEREQkdygQi1zH7bfDAw9ASgq8/bbV1YiIiEhuUCAWyUTqVeKZM2HfPmtrERERkZynQCySiTvugNatITlZV4lFRETyIwVikSwYOtT8+O9/w/791tYiIiIiOUuBWCQLGjWCFi3gyhUYM8bqakRERCQnKRCLZFHqVeLp0+HgQWtrERERkZyjQCySRY0bQ9OmkJQEY8daXY2IiIjkFAVikRuQepX400/h8GFraxEREZGcoUAscgPuvRfuuQcSE+Gdd6yuRkRERHKCArHIDUq9SjxlChw7Zm0tIiIicvMUiEVu0H33wV13QUICvPuu1dWIiIjIzVIgFrlBDsc/V4knT4bjx62tR0RERG6OArFINrRsaT7B7tIlGDfO6mpERETkZigQi2SDwwHDhpnrn3wCJ09aW4+IiIhknwKxSDa1aQP168PFizB+vNXViIiISHYpEItk09VjiT/+GP76y9p6REREJHsUiEVuwoMPQp06cP48TJhgdTUiIiKSHQrEIjfh6qvEH34If/9tbT0iIiJy4xSIRW5S+/Zw220QHw8ffGB1NSIiInKjFIhFbpKXF7zxhrk+YQKcPWtpOSIiInKDFIhFckDHjlCjhhmGP/rI6mpERETkRigQi+QALy94/XVzffx4OHfO2npEREQk6xSIRXLIo49C1armjXUff2x1NSIiIpJVCsQiOcTb+5+rxO+9Z07FJiIiIp5PgVgkB3XpAlWqmA/pmDTJ6mpEREQkKxSIRXKQjw+89pq5/u675mOdRURExLMpEIvksO7doWJFOHkS/vUvq6sRERGRzCgQi+QwX18YMsRcf+cduHTJ2npERETk+hSIRXJBjx5QoQLExcHUqVZXIyIiItdjeSCeOHEi4eHhBAQEEBERwaZNmzLcd8eOHXTs2JHw8HAcDgcTJkzI1jEvX75Mv379KFGiBAULFqRjx44cP348J9sSm/Pzg8GDzfWxY+HyZWvrERERkYxZGojnzZtHdHQ0w4YNY9u2bdSpU4dWrVpx4sSJdPe/ePEilSpVYsyYMYSEhGT7mC+++CLffPMN8+fPZ/Xq1Rw9epQOHTrkSo9iX716QdmycPQoTJtmdTUiIiKSEUsD8fjx4+nbty+9e/emRo0aTJ48maCgIKZlkB4aNmzIu+++S5cuXfD398/WMc+ePctnn33G+PHjue+++6hfvz7Tp09n3bp1bNiwIdd6Ffvx94dXXzXXR4+GhARr6xEREZH0+Vh14sTERLZu3crg1N8rA15eXjRv3pz169fn2jG3bt1KUlISzZs3d+5TrVo1ypcvz/r167nzzjvTPXZCQgIJVyWa+Ph4AJKSkkhKSspWvTci9RzuOJenycu99+gBb7/tw+HDDqZNu8KTTxpZfm9e7vtm2bV3u/YN6v3qj3Zh175BvV/90V3ny4xlgfjUqVMkJycTHBzssj04OJhdu3bl2jHj4uLw8/OjaNGiafaJi4vL8NijR49mxIgRabYvW7aMoKCgbNWbHTExMW47l6fJq73ff38lPv30NoYNS6RUqeX4+mY9FEPe7Tsn2LV3u/YN6t2O7No3qHd3uJjFBwJYFojzmsGDBxMdHe38PD4+nnLlytGyZUsKFy6c6+dPSkoiJiaGFi1a4Ovrm+vn8yR5vfemTeGbbwyOHw/i9On76d07a4E4r/d9M+zau137BvVux97t2jeod3f2nvob/cxYFohLliyJt7d3mtkdjh8/nuENczlxzJCQEBITEzlz5ozLVeLMzuvv75/uuGVfX1+3/mF29/k8SV7t3dcXXnkFXnoJxozxoXdvc1vW3583+84Jdu3drn2Derdj73btG9S7O3rP6jksu6nOz8+P+vXrs2LFCue2lJQUVqxYQaNGjXLtmPXr18fX19dln927d3Pw4MFsn1ckM08/DaVKwf79MHu21dWIiIjI1SwdMhEdHU3Pnj1p0KABd9xxBxMmTODChQv07t0bgB49elCmTBlGjx4NmDfN/f777871I0eOEBsbS8GCBalSpUqWjlmkSBH69OlDdHQ0xYsXp3Dhwjz//PM0atQowxvqRG5WgQIwcCAMGgRvvWU+3tlHA5ZEREQ8gqX/JHfu3JmTJ08ydOhQ4uLiqFu3LkuWLHHeFHfw4EG8vP65iH306FHq1avn/HzcuHGMGzeOyMhIVq1alaVjArz//vt4eXnRsWNHEhISaNWqFZ988ol7mhbbeu4581HOe/bA3Lnw2GNWVyQiIiLgATfVRUVFERUVle5rqSE3VXh4OIaR+Q1J1zsmQEBAABMnTmTixIk3VKvIzShY0BxHPGQIvPkmdO0K3t5WVyUiIiKWP7pZxE769YNixWD3bpg/3+pqREREBBSIRdyqcGF48UVzfdQoSEmxth4RERFRIBZxu+efhyJF4PffYeFCq6sRERERBWIRNytaFPr3N9dHjtRVYhEREaspEItYYMAAKFQIfv0VvvrK6mpERETsTYFYxALFisELL5jrI0dCFiZPERERkVyiQCxikRdfNB/YERsL335rdTUiIiL2pUAsYpESJSB1umxdJRYREbGOArGIhV56CYKCYMsW+P57q6sRERGxJwViEQuVKmU+0hl0lVhERMQqCsQiFhs4EAICYONGiImxuhoRERH7USAWsVhwMDzzjLk+YoSuEouIiLibArGIB3j5ZfD3h3XrYOVKq6sRERGxFwViEQ8QFgZ9+5rrI0ZYW4uIiIjdKBCLeIhBg8DPD9asgdWrra5GRETEPhSIRTxE2bLQp4+5PnKktbWIiIjYiQKxiAd59VXw9YUffoCJE71Ys6YMq1c7SE62ujIREZH8S4FYxIOULw+Rkeb6iy96M358A1q08CE8HBYutLQ0ERGRfEuBWMSDLFwIK1ak3X7kCHTqpFAsIiKSGxSIRTxEcjL075/+PMSp2wYMQMMnREREcpgCsYiHWLsWDh/O+HXDgEOHzP1EREQk5ygQi3iIY8dydj8RERHJGgViEQ8RGpqz+4mIiEjWKBCLeIgmTcy5iB2OjPfx8gJvb/fVJCIiYgcKxCIewtsbPvjAXL82FKd+npICzZrBZ5+5tzYREZH8TIFYxIN06AALFkCZMq7by5aFmTOhY0dISoInn4TnnzfXRURE5OYoEIt4mA4d4MABiIm5QnT0FmJirrB/P3TvDl9++c9jnT/+GFq1glOnLC1XREQkz1MgFvFA3t4QGWlwzz1HiIw0nOOGvbzgjTdg8WIoWBBWroSGDeGXXywtV0REJE9TIBbJg9q1g/XroVIl82pyo0bwn/9YXZWIiEjepEAskkfVqgWbN0Pz5nDxovlo56FDzRvvREREJOsUiEXysOLF4fvv4cUXzc9HjTLHIJ87Z21dIiIieYkCsUge5+MD48fD55+Dnx989ZU5hGLfPqsrExERyRsUiEXyiZ49Yc0a80l2O3aYN9vFxFhdlYiIiOdTIBbJRyIiYMsW8+Pff0Pr1vD++2AYVlcmIiLiuRSIRfKZsDBYtcq8YpySAtHR0Ls3XL5sdWUiIiKeSYFYJB8KCIDp082rw15eMGMGREbC0aNWVyYiIuJ5FIhF8imHAwYMgKVLoVgx2LQJGjSADRusrkxERMSzKBCL5HPNm5vzFdesCceOmVeKZ8ywuioRERHPoUAsYgOVK5tPtmvXDhIToVcvc+7iK1esrkxERMR6CsQiNlGoECxcaD7NDmDCBHMWir/+srQsERERyykQi9iIlxeMGAELFkCBArBiBdxxB/z2m9WViYiIWEeBWMSGOnaEdesgPBz+9z/zyXaLF1tdlYiIiDUUiEVsqnZt82a7pk3h/Hl4+GEYOdKcu1hERMROFIhFbKxkSXNatuefNz8fNgweecQMyCIiInahQCxic76+8OGH8Omn5vrChXDXXeZQChERETtQIBYRAPr0MR/5HBwMv/4KDRvCDz9YXZWIiEjuUyAWEae77oItW8wn2p0+DS1bwkcfgWFYXZmIiEjuUSAWERdly8KaNfDYY5CcDC+8AE8+CQkJVlcmIiKSOzwiEE+cOJHw8HACAgKIiIhg06ZN191//vz5VKtWjYCAAG677Ta+++47l9cdDke6y7vvvuvcJzw8PM3rY8aMyZX+RPKawED4979h3Dhz7uJp08zZKOLirK5MREQk51keiOfNm0d0dDTDhg1j27Zt1KlTh1atWnHixIl091+3bh1du3alT58+bN++nfbt29O+fXt+u+rJAseOHXNZpk2bhsPhoGPHji7HGjlypMt+z6feai8iOBzw0kvw3XdQtKj56OcGDcyp2kRERPITywPx+PHj6du3L71796ZGjRpMnjyZoKAgpk2blu7+H3zwAa1bt+bll1+mevXqjBo1ittvv52PP/7YuU9ISIjL8tVXX9G0aVMqVarkcqxChQq57FegQIFc7VUkL2rVCjZtgurV4cgRaNIEZs60uioREZGc42PlyRMTE9m6dSuDBw92bvPy8qJ58+asX78+3fesX7+e6Ohol22tWrVicQaP2Tp+/Dj//e9/mTFjRprXxowZw6hRoyhfvjzdunXjxRdfxMcn/S9JQkICCVcNooyPjwcgKSmJpKSk6/aZE1LP4Y5zeRq79u5JfYeHw9q10KOHN99958Xjj8P27cm89VYK3t45fz5P6t2d7No3qPerP9qFXfsG9X71R3edLzOWBuJTp06RnJxMcHCwy/bg4GB27dqV7nvi4uLS3T8ug8GNM2bMoFChQnTo0MFl+wsvvMDtt99O8eLFWbduHYMHD+bYsWOMHz8+3eOMHj2aESNGpNm+bNkygoKCMuwxp8XExLjtXJ7Grr17Ut9PPgkFClRj/vyqjB/vzQ8//MXAgVsoWDB3frB5Uu/uZNe+Qb3bkV37BvXuDhcvXszSfpYGYneYNm0a3bt3JyAgwGX71VeZa9eujZ+fH08//TSjR4/G398/zXEGDx7s8p74+HjKlStHy5YtKVy4cO418P+SkpKIiYmhRYsW+Pr65vr5PIlde/fUvh94ANq3v0Lfvt7ExpZm+PA2/Oc/V6hePefO4am95za79g3q3Y6927VvUO/u7D31N/qZsTQQlyxZEm9vb44fP+6y/fjx44SEhKT7npCQkCzvv3btWnbv3s28efMyrSUiIoIrV65w4MABqlatmuZ1f3//dIOyr6+vW/8wu/t8nsSuvXti3926QY0a0K4d7N3roHFjX2bNggcfzNnzeGLv7mDXvkG927F3u/YN6t0dvWf1HJbeVOfn50f9+vVZsWKFc1tKSgorVqygUaNG6b6nUaNGLvuDedk9vf0/++wz6tevT506dTKtJTY2Fi8vL0qXLn2DXYjYU9265kM87rkHzp0zw/Hbb+shHiIikvdYPstEdHQ0U6dOZcaMGezcuZNnn32WCxcu0Lt3bwB69OjhctNd//79WbJkCe+99x67du1i+PDhbNmyhaioKJfjxsfHM3/+fJ588sk051y/fj0TJkzg559/5n//+x+zZs3ixRdf5LHHHqNYsWK527BIPlKqFCxfDs89Zwbh116Dzp3hwgWrKxMREck6y8cQd+7cmZMnTzJ06FDi4uKoW7cuS5Yscd44d/DgQby8/sntd911F7Nnz+b1119nyJAh3HLLLSxevJhatWq5HHfu3LkYhkHXrl3TnNPf35+5c+cyfPhwEhISqFixIi+++GKa2StEJHO+vjBxItSpA1FRMH8+7NkDixdDhQpWVyciIpI5ywMxQFRUVJorvKlWrVqVZtsjjzzCI488ct1jPvXUUzz11FPpvnb77bezYcOGG65TRDL21FPmuOIOHSA21nyIx4IFEBlpdWUiIiLXZ/mQCRHJPxo3NscV3347nDoFzZvDpEkaVywiIp5NgVhEclT58uZDPLp2hStXzPHFzzwDiYlWVyYiIpI+BWIRyXFBQTBrFowZAw4HTJkCzZrBNTMmioiIeAQFYhHJFQ4HDBoE334LhQvDjz9Cw4awbZvVlYmIiLhSIBaRXHX//bBpE9x6Kxw6BHffDXPmWF2ViIjIPxSIRSTXVa0KGzea4fjyZfNJd6++CsnJVlcmIiKiQCwiblK0KHz9tTmMAmDsWHjoITh71tKyREREFIhFxH28vc0b7WbPhoAA+O47iIiA3butrkxEROxMgVhE3K5rV/Mmu7JlzTB8xx1mOAZzGMXq1Q7WrCnD6tUODasQEZFcp0AsIpaoX998iEfjxhAfDw88AI8/DuHh0KKFD+PHN6BFCx/Cw2HhQqurFRGR/EyBWEQsExwMK1aYj302DJg5Ew4fdt3nyBHo1EmhWEREco8CsYhYys8PJk40b7pLT+pjnwcM0KwUIiKSOxSIRcRyP/4IZ85k/LphmHMYr13rtpJERMRGFIhFxHLHjmVtvz/+yN06RETEnhSIRcRyoaFZ2y8qCnr0gHXr/hlKISIicrMUiEXEck2amFOwORwZ7+PrC0lJ8MUX5uOf69aFSZPMGSpERERuhgKxiFjO2xs++MBcvzYUOxzmMmeO+fjn3r3Nh3r88gs89xyEhcHTT8P27e6vW0RE8gcFYhHxCB06wIIFUKaM6/ayZc3tHTuaD/CYNg2OHoUJE6BaNbhwAaZMgdtvhzvvhM8/h4sXrehARETyKgViEfEYHTrAgQMQE3OF6OgtxMRcYf9+c/vVihWD/v3h999h1Sro0sUcUpF6BblMGXOatl27LGhCRETyHAViEfEo3t4QGWlwzz1HiIw08PbOeF+HAyIjzeEUhw7B6NHmk+7OnDGHYFSvDk2bwrx5kJjorg5ERCSvUSAWkXwhOBhefRX27YPvv4eHHgIvr3+uIJcrB0OGwP79VlcqIiKeRoFYRPIVLy9o3Rq++socfjF0qDmt24kT5hXkypXh/vvh66/hyhWrqxUREU+gQCwi+Va5cjBiBPz5J/znP9CihTl/8fffQ7t2ULEijBxp3qQnIiL2pUAsIvmer695Y96yZbBnD7z8MpQoAYcPw7BhUL68OYtFTAykpFhdrYiIuJsCsYjYSpUq8M47ZhieORMaN4bkZFi4EFq2hKpVYdw4OHXK6kpFRMRdFIhFxJYCAqB7d1i7Fn791XwsdOHCsHeveQW5TBl47DH48Uc9JlpEJL9TIBYR26tVCz76CI4cgalTzYd8JCbCrFnmY6Vr14aJE+HsWasrFRGR3KBALCLy/woWhCefhK1bYdMmeOIJCAyE334zryCHhUHfvrBtm9WViohITlIgFhFJR8OG8Nln5gwUH34INWqYj4T+9FOoX/+fx0jrMdEiInmfArGIyHUULQrPP29eJV69Grp2NWet2LwZ+vQxrxqnPkZaRETyJgViEZEscDjgnntg9mxzhoqxY6FSJXNc8YcfQs2a/zxGOiHB6mpFRORGKBCLiNyg0qXhlVfMOY2XLIH27c0n5K1ZA926mQ8EefVV+N//rK5URESyQoFYRCSbvLygVStYtMh8Gt7w4eZ0bSdPmleQK1c2HyO9eLEeEy0i4skUiEVEckDZsuZT7w4cMANyq1bm9qVL4eGHITzcfIz0kSPpvz85GVavdrBmTRlWr3aQnOyuykVERIFYRCQH+fiYQyiWLDEf8vHKK1CypBmEhw+HChXMgLx06T+PiV640AzMLVr4MH58A1q08CE83NwuIiK5T4FYRCSXVK5sDp04fNi8Ge+ee8wrwYsXm0MpbrkFevSATp3Mfa525Ii5XaFYRCT3KRCLiOQyf39zurbVq83p255/3nxM9P/+B198kf6joVO3DRiAhk+IiOQyBWIRETeqWdOcpu3oUXj55evvaxhw6BCsXeue2kRE7MrH6gJEROyoQAGoVy9r+44dC3//bQ65KFEid+sSEbEjBWIREYuEhmZtvyVLzAWgVi3zASCRkWZADg7OvfpEROxCgVhExCJNmpjTtR05kv44YocDiheHjh3NYRM7d5pjkH/7DSZONPepVs0MxqkhuUwZ9/YgIpIfKBCLiFjE2xs++MCcTcLhcA3FDof5ccoU6NDBXD9xwnwa3po15g16v/wCu3aZy5Qp5j6VK7teQQ4Pd2tLIiJ5kgKxiIiFOnSABQugf3/XqdfKloUJE/4Jw2A+MrpTJ3MBOH3avHK8erW5xMbCvn3mMm2auU/58v8E5MhIMzCnhm0RETEpEIuIWKxDB2jXDlauvML338fSpk1dmjb1wdv7+u8rXtx8X7t25udnz8JPP/0TkLdsgYMHzandvvjC3CcszHWIRbVqCsgiIgrEIiIewNsbIiMNLlw4QmRknUzDcHqKFIH77zcXgPPnYd26fwLypk3mdG9z55oLmFedrw7INWuClybkFBGbUSAWEcmnChaEli3NBeDSJdiw4Z+AvGGDOS55wQJzAfOqc5Mm/wTkOnXIVjgXEclLFIhFRGwiMBCaNjUXgIQE86px6k16P/1kjkv+6itzAfOqc+PG/1xFvv128PW1rgcRkdzgEb8YmzhxIuHh4QQEBBAREcGmTZuuu//8+fOpVq0aAQEB3HbbbXz33Xcur/fq1QuHw+GytG7d2mWf06dP0717dwoXLkzRokXp06cP58+fz/HeREQ8lb+/eTX4tddg2TI4cwbWr4cxY6BNGyhUyByX/N//wqBBcOedUKwYtGoFb79tBujERKu7EBG5eZYH4nnz5hEdHc2wYcPYtm0bderUoVWrVpw4cSLd/detW0fXrl3p06cP27dvp3379rRv357ffvvNZb/WrVtz7Ngx5zJnzhyX17t3786OHTuIiYnh22+/Zc2aNTz11FO51qeIiKfz9TVD76BB8N135tXiLVvgvffgoYfMMHzhghmeX3vNvHJcpAjcdx+MGAGrVsHly1Z3ISJy4ywPxOPHj6dv37707t2bGjVqMHnyZIKCgpiWOmfQNT744ANat27Nyy+/TPXq1Rk1ahS33347H3/8sct+/v7+hISEOJdixYo5X9u5cydLlizh008/JSIigsaNG/PRRx8xd+5cjh49mqv9iojkFT4+UL8+REebQyhOnTKndvvwQ/NhIaVKmQF45UoYPtwcilGkiDm84o03YPlyM0BnJjkZVq92sGZNGVavdpCcnNudiYi4snQMcWJiIlu3bmXw4MHObV5eXjRv3pz169en+57169cTHR3tsq1Vq1YsXrzYZduqVasoXbo0xYoV47777uPNN9+kRIkSzmMULVqUBg0aOPdv3rw5Xl5ebNy4kYcffjjNeRMSEkhISHB+Hh8fD0BSUhJJSUk31ng2pJ7DHefyNHbt3a59g317zwt916hhLs88Yz5IZOdOWLvWizVrHKxd6yAuzsHateb8yG++CT4+BvXrGzRpYnDPPQZ33WVQuPA/x1u0yEF0tDdHjvgADRg/HsqUMRg/PpmHH07n8X35UF74vucGu/YN6v3qj+46X2YsDcSnTp0iOTmZ4OBgl+3BwcHs2rUr3ffExcWlu39cXJzz89atW9OhQwcqVqzIvn37GDJkCG3atGH9+vV4e3sTFxdH6dKlXY7h4+ND8eLFXY5ztdGjRzNixIg025ctW0ZQUFCW+s0JMTExbjuXp7Fr73btG+zbe17ru1w56N4dunWDY8cKsGNHCX77rSQ7dpTg1KkgNm50sHEjjBsHXl4GlSqdoUaNv/DxSWHhwlvSHO/IEejc2ZtBgzbTqNExCzqyRl77vucUu/YN6t0dLl68mKX98uUsE126dHGu33bbbdSuXZvKlSuzatUqmjVrlq1jDh482OXKdHx8POXKlaNly5YUvvpyRy5JSkoiJiaGFi1a4GuzW7zt2rtd+wb79p7f+jYMOHAgibVrHaxd68XatQ7+9z8He/cWY+/e1GFsBnDtk0EcOBwGs2Y1ZPjwK/l+2rf89n3PKrv2Derdnb2n/kY/M5YG4pIlS+Lt7c3x48ddth8/fpyQkJB03xMSEnJD+wNUqlSJkiVLsnfvXpo1a0ZISEiam/auXLnC6dOnMzyOv78//v7+abb7+vq69Q+zu8/nSezau137Bvv2np/6vvVWc+nTx/z88GFzirc5c8zZK9KGYZNhODh8GPr29eX++81hGlWrQkCA20p3u/z0fb8Rdu0b1Ls7es/qOSy9qc7Pz4/69euzYsUK57aUlBRWrFhBo0aN0n1Po0aNXPYH87J7RvsDHD58mL/++ovQ0FDnMc6cOcPWrVud+/zwww+kpKQQERFxMy2JiMh1lC1rDq/o3j1r+8+caQ7FqFsXChSAW24xH1U9eLD5OOqtWyGLvxEVEcmQ5UMmoqOj6dmzJw0aNOCOO+5gwoQJXLhwgd69ewPQo0cPypQpw+jRowHo378/kZGRvPfee7Rt25a5c+eyZcsWpkyZAsD58+cZMWIEHTt2JCQkhH379vHKK69QpUoVWrVqBUD16tVp3bo1ffv2ZfLkySQlJREVFUWXLl0ICwuz5gshImIj/399IlMPPGDOj7xjB/z9N+zday5ff/3PPg4HhIf/c8Nf6lK9ujmXsohIZiwPxJ07d+bkyZMMHTqUuLg46taty5IlS5w3zh08eBAvr38uZN91113Mnj2b119/nSFDhnDLLbewePFiatWqBYC3tze//PILM2bM4MyZM4SFhdGyZUtGjRrlMuRh1qxZREVF0axZM7y8vOjYsSMffvihe5sXEbGpJk3Mq8VHjphjja/lcJivL15sPjraMOD4cfj997TLyZOwf7+5mMMw/lGunBmOa9Z0DcpFi7qjSxHJKywPxABRUVFERUWl+9qqVavSbHvkkUd45JFH0t0/MDCQpUuXZnrO4sWLM3v27BuqU0REcoa3N3zwAXTqZIbfq0Ox4/+HFU+YgPOGOocDQkLM5b77XI918qQ5/dvvv5tXklODclwcHDpkLtf+sxAW5no1OTUwFy+eay2LiAfziEAsIiL206EDLFgA/fubN9ulKlvWDMMdOmTtOKVKmcs997huP336n6B89XL4MBw9ai7Ll7u+Jzg47dCLGjXgmpk6RSSfUSAWERHLdOhg3iS3cuUVvv8+ljZt6tK0qU+OTLVWvDjcfbe5XO3s2fSD8p9/msMyjh83n753tZIl0w/KISH/XNHOjquf0leggIOmTcn308yJeCIFYhERsZS3N0RGGly4cITIyDq5HgiLFIE77zSXq50/D7t2pR16sX+/+djqNWvM5WpFi6YddlGjBpQpk3lQXrgw9er4P0/pK1vWHEqS1avjIpIzFIhFRESAggWhQQNzudrFi7B7t+vV5B07YN8+cwaMdevM5WqFCqW9mlyzpnmTn5eXGYY7dUp7Q+GRI+b2BQsUikXcSYFYRETkOoKCoF49c7na5cvwxx9ph17s2QPnzsHGjeZytQIFoFo1c8hGerNrGIZ5ZXnAAHMoiYZPiLiHArGIiEg2BARA7drmcrXERHOu5KuHXfz+u3mV+cIF82Ei12MY5swYL78MzZtDhQpQvrzmVBbJTQrEIiIiOcjP759hEle7csUcZvGvf8H772d+nPffd92vWDEzGJcv/09IvvpjcLA5HENEbpwCsYiIiBv4+EDVqvDQQ1kLxHfdZV5RPnjQfEpf6vLzz+nv7+dnjlHOKDSXL29e1RaRtBSIRURE3CirT+lbs+afMcTnzpnB+M8/XT+mrh85Yg7V2LfPXDJSurRrQL42NJcocXPTyGWVppsTT6NALCIi4kY3+pQ+MMcP16xpLum5csUMxdcLzRcuwIkT5rJ5c/rHCQq6/rCMMmXA1/fm+td0c+KJFIhFRETcLKee0pfKx8cMrBUqmFegr2UY5nCLa8Py1aE5Ls6cYm7XLnNJj5eX+djr64XmwoUzrlPTzYmnUiAWERGxQG4+pe9aDof55L7ixdNOH5fq8mUznF8vNCcmmvscPpx27uVURYqkH5bLloWoKE03J55JgVhERMQi7n5K3/UEBECVKuaSnpQUc7hFemE59ePp0+ajsX/91VxuROp0czNmwAMPmI/L1qwZ4i4KxCIiIpIpLy8ICTGXO+5If5/z513HLV/98fff4a+/Mj9Pnz7mR29vKFXKnE4uJMT8mNF6iRIKz3JzFIhFREQkRxQsmP4czACrVkHTppkfo3BhiI83Z6KIizOXjKaaS+Xtbc6gcb3QnLpevLi14VkzbHgmBWIRERHJdVmdbm7/fnN4xsmTZhg+ftxc0luPizOHaSQnw7Fj5pIZHx/zynNmV52Dg3M+PGuGDc+lQCwiIiK57kamm/P2NmezCAvL/LiJiWnDc0YB+vRpc4q6GwnPqVeeU4Py9cLz9eZw1gwbnk2BWERERNwip6ebA/MJfWXKmEtmEhPNGwPTu9J87ba//zbD89Gj5pIZX9+04Tl1vVQpcwYNzbDhuRSIRURExG3cOd3ctfz8zPBdtmzm+yYkpB+e01s/cwaSksyrvUeO3HhdqTNsPP443H67ebW5RAnXpXhx84q15A59aUVERMStPGm6uYz4+0O5cuaSmdTwnFFo/uWXjB92crU5c8wlI0WKpB+Wrw7N124rVMg9j+POCk++oVCBWEREROQmZBaeszrDRocOEBhojnX+669/ljNnzNfPnjWX/fuzXpuPT8Zh+Xrb/fyyfo6s8PQbChWIRURERHJRVmfY+PLL9K+YXrlijmm+NiinLhltv3zZfG/qleobUbDgjV+NLlIk/Vk58sINhQrEIiIiIrnoRmbYSE/qVHGlSt3YeS9dynp4Tt1++rQ57d358/88aCWrvLygWDHXkFysGCxa5Pk3FCoQi4iIiOSy3JhhIzOBgVm/iTBVSoo5LCOrATp1OX/efG/q51mVekPh2rVw77033GKOUSAWERERcQMrZ9jIqtSrvMWKQZUqWX9fQoIZkq8NyitWwNy5mb8/K/NC5yYFYhERERE3yQszbGSHvz+EhprL1apUyVogvvZ97mbh07xFREREJD9LvaEwo6nfHA5zdo4mTdxb17UUiEVEREQkV6TeUAhpQ3FWbih0FwViEREREck1qTcUXvt47bJlPWPKNdAYYhERERHJZZ5+Q6ECsYiIiIjkOk++oVBDJkRERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1nysLiCvMgwDgPj4eLecLykpiYsXLxIfH4+vr69bzukp7Nq7XfsG+/Zu175Bvduxd7v2Derdnb2n5rTU3JYRBeJsOnfuHADlypWzuBIRERERuZ5z585RpEiRDF93GJlFZklXSkoKR48epVChQjgcjlw/X3x8POXKlePQoUMULlw418/nSezau137Bvv2bte+Qb3bsXe79g3q3Z29G4bBuXPnCAsLw8sr45HCukKcTV5eXpQtW9bt5y1cuLDt/vKksmvvdu0b7Nu7XfsG9W7H3u3aN6h3d/V+vSvDqXRTnYiIiIjYmgKxiIiIiNiaAnEe4e/vz7Bhw/D397e6FLeza+927Rvs27td+wb1bsfe7do3qHdP7F031YmIiIiIrekKsYiIiIjYmgKxiIiIiNiaArGIiIiI2JoCsYiIiIjYmgKxh1uzZg0PPvggYWFhOBwOFi9ebHVJbjF69GgaNmxIoUKFKF26NO3bt2f37t1Wl+UWkyZNonbt2s5Jyxs1asT3339vdVluN2bMGBwOBwMGDLC6lFw3fPhwHA6Hy1KtWjWry3KbI0eO8Nhjj1GiRAkCAwO57bbb2LJli9Vl5arw8PA033OHw0G/fv2sLi3XJScn88Ybb1CxYkUCAwOpXLkyo0aNwg73+J87d44BAwZQoUIFAgMDueuuu9i8ebPVZeW4zLKLYRgMHTqU0NBQAgMDad68OXv27LGm2P+nQOzhLly4QJ06dZg4caLVpbjV6tWr6devHxs2bCAmJoakpCRatmzJhQsXrC4t15UtW5YxY8awdetWtmzZwn333Ue7du3YsWOH1aW5zebNm/nXv/5F7dq1rS7FbWrWrMmxY8ecy48//mh1SW7x999/c/fdd+Pr68v333/P77//znvvvUexYsWsLi1Xbd682eX7HRMTA8AjjzxicWW5b+zYsUyaNImPP/6YnTt3MnbsWN555x0++ugjq0vLdU8++SQxMTF88cUX/Prrr7Rs2ZLmzZtz5MgRq0vLUZlll3feeYcPP/yQyZMns3HjRgoUKECrVq24fPmymyu9iiF5BmAsWrTI6jIsceLECQMwVq9ebXUplihWrJjx6aefWl2GW5w7d8645ZZbjJiYGCMyMtLo37+/1SXlumHDhhl16tSxugxLDBo0yGjcuLHVZViuf//+RuXKlY2UlBSrS8l1bdu2NZ544gmXbR06dDC6d+9uUUXucfHiRcPb29v49ttvXbbffvvtxmuvvWZRVbnv2uySkpJihISEGO+++65z25kzZwx/f39jzpw5FlRo0hViyRPOnj0LQPHixS2uxL2Sk5OZO3cuFy5coFGjRlaX4xb9+vWjbdu2NG/e3OpS3GrPnj2EhYVRqVIlunfvzsGDB60uyS2+/vprGjRowCOPPELp0qWpV68eU6dOtbost0pMTGTmzJk88cQTOBwOq8vJdXfddRcrVqzgjz/+AODnn3/mxx9/pE2bNhZXlruuXLlCcnIyAQEBLtsDAwNt8xshgP379xMXF+fyM75IkSJERESwfv16y+rysezMIlmUkpLCgAEDuPvuu6lVq5bV5bjFr7/+SqNGjbh8+TIFCxZk0aJF1KhRw+qyct3cuXPZtm1bvhxTdz0RERF8/vnnVK1alWPHjjFixAiaNGnCb7/9RqFChawuL1f973//Y9KkSURHRzNkyBA2b97MCy+8gJ+fHz179rS6PLdYvHgxZ86coVevXlaX4havvvoq8fHxVKtWDW9vb5KTk3nrrbfo3r271aXlqkKFCtGoUSNGjRpF9erVCQ4OZs6cOaxfv54qVapYXZ7bxMXFARAcHOyyPTg42PmaFRSIxeP169eP3377zVb/g65atSqxsbGcPXuWBQsW0LNnT1avXp2vQ/GhQ4fo378/MTExaa6g5HdXXxmrXbs2ERERVKhQgS+//JI+ffpYWFnuS0lJoUGDBrz99tsA1KtXj99++43JkyfbJhB/9tlntGnThrCwMKtLcYsvv/ySWbNmMXv2bGrWrElsbCwDBgwgLCws33/Pv/jiC5544gnKlCmDt7c3t99+O127dmXr1q1Wl2Z7GjIhHi0qKopvv/2WlStXUrZsWavLcRs/Pz+qVKlC/fr1GT16NHXq1OGDDz6wuqxctXXrVk6cOMHtt9+Oj48PPj4+rF69mg8//BAfHx+Sk5OtLtFtihYtyq233srevXutLiXXhYaGpvmPXvXq1W0zZOTPP/9k+fLlPPnkk1aX4jYvv/wyr776Kl26dOG2227j8ccf58UXX2T06NFWl5brKleuzOrVqzl//jyHDh1i06ZNJCUlUalSJatLc5uQkBAAjh8/7rL9+PHjztesoEAsHskwDKKioli0aBE//PADFStWtLokS6WkpJCQkGB1GbmqWbNm/Prrr8TGxjqXBg0a0L17d2JjY/H29ra6RLc5f/48+/btIzQ01OpSct3dd9+dZkrFP/74gwoVKlhUkXtNnz6d0qVL07ZtW6tLcZuLFy/i5eUaP7y9vUlJSbGoIvcrUKAAoaGh/P333yxdupR27dpZXZLbVKxYkZCQEFasWOHcFh8fz8aNGy29V0ZDJjzc+fPnXa4S7d+/n9jYWIoXL0758uUtrCx39evXj9mzZ/PVV19RqFAh57iiIkWKEBgYaHF1uWvw4MG0adOG8uXLc+7cOWbPns2qVatYunSp1aXlqkKFCqUZI16gQAFKlCiR78eODxw4kAcffJAKFSpw9OhRhg0bhre3N127drW6tFz34osvctddd/H222/z6KOPsmnTJqZMmcKUKVOsLi3XpaSkMH36dHr27ImPj33+OX7wwQd56623KF++PDVr1mT79u2MHz+eJ554wurSct3SpUsxDIOqVauyd+9eXn75ZapVq0bv3r2tLi1HZZZdBgwYwJtvvsktt9xCxYoVeeONNwgLC6N9+/bWFW3Z/BaSJStXrjSANEvPnj2tLi1XpdczYEyfPt3q0nLdE088YVSoUMHw8/MzSpUqZTRr1sxYtmyZ1WVZwi7TrnXu3NkIDQ01/Pz8jDJlyhidO3c29u7da3VZbvPNN98YtWrVMvz9/Y1q1aoZU6ZMsbokt1i6dKkBGLt377a6FLeKj483+vfvb5QvX94ICAgwKlWqZLz22mtGQkKC1aXlunnz5hmVKlUy/Pz8jJCQEKNfv37GmTNnrC4rx2WWXVJSUow33njDCA4ONvz9/Y1mzZpZ/vfAYRg2eDSMiIiIiEgGNIZYRERERGxNgVhEREREbE2BWERERERsTYFYRERERGxNgVhEREREbE2BWERERERsTYFYRERERGxNgVhEREREbE2BWEREborD4WDx4sVWlyEikm0KxCIieVivXr1wOBxpltatW1tdmohInuFjdQEiInJzWrduzfTp0122+fv7W1SNiEjeoyvEIiJ5nL+/PyEhIS5LsWLFAHM4w6RJk2jTpg2BgYFUqlSJBQsWuLz/119/5b777iMwMJASJUrw1FNPcf78eZd9pk2bRs2aNfH39yc0NJSoqCiX10+dOsXDDz9MUFAQt9xyC19//XXuNi0ikoMUiEVE8rk33niDjh078vPPP9O9e3e6dOnCzp07Abhw4QKtWrWiWLFibN68mfnz57N8+XKXwDtp0iT69evHU089xa+//srXX39NlSpVXM4xYsQIHn30UX755Rfuv/9+unfvzunTp93ap4hIdjkMwzCsLkJERLKnV69ezJw5k4CAAJftQ4YMYciQITgcDp555hkmTZrkfO3OO+/k9ttv55NPPmHq1KkMGjSIQ4cOUaBAAQC+++47HnzwQY4ePUpwcDBlypShd+/evPnmm+nW4HA4eP311xk1ahRghuyCBQvy/fffayyziOQJGkMsIpLHNW3a1CXwAhQvXty53qhRI5fXGjVqRGxsLAA7d+6kTp06zjAMcPfdd5OSksLu3btxOBwcPXqUZs2aXbeG2rVrO9cLFChA4cKFOXHiRHZbEhFxKwViEZE8rkCBAmmGMOSUwMDALO3n6+vr8rnD4SAlJSU3ShIRyXEaQywiks9t2LAhzefVq1cHoHr16vz8889cuHDB+fpPP/2El5cXVatWpVChQoSHh7NixQq31iwi4k66QiwiksclJCQQFxfnss3Hx4eSJUsCMH/+fBo0aEDjxo2ZNWsWmzZt4rPPPgOge/fuDBs2jJ49ezJ8+HBOnjzJ888/z+OPP05wcDAAw4cP55lnnqF06dK0adOGc+fO8dNPP/H888+7t1ERkVyiQCwiksctWbKE0NBQl21Vq1Zl165dgDkDxNy5c3nuuecIDQ1lzpw51KhRA4CgoCCWLl1K//79adiwIUFBQXTs2JHx48c7j9WzZ08uX77M+++/z8CBAylZsiSdOnVyX4MiIrlMs0yIiORjDoeDRYsW0b59e6tLERHxWBpDLCIiIiK2pkAsIiIiIramMcQiIvmYRsWJiGROV4hFRERExNYUiEVERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1hSIRURERMTWFIhFRERExNYUiEVERETE1v4PcISQBz/ueWsAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def plot_best_loss_curve(loss_list, best_lr, best_hl):\n",
        "    \"\"\"\n",
        "    Plots the loss curve for the best model using the given loss history.\n",
        "\n",
        "    Parameters:\n",
        "        loss_list (list of floats): The list containing the loss values after each epoch for the best model.\n",
        "        best_lr (float): The learning rate used for the best model.\n",
        "        best_hl (int): The hidden layer size used for the best model.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(range(1, len(loss_list) + 1), loss_list, marker='o', linestyle='-', color='blue')\n",
        "    plt.title(f\"Training Loss Curve for Best Model (lr={best_lr}, hl={best_hl})\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xticks(range(1, len(loss_list) + 1))\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Plot the loss curve for the best model\n",
        "plot_best_loss_curve(best_loss_list, best_lr, best_hl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDGgmS7Zrp48"
      },
      "source": [
        "# 3 Implementation in PyTorch\n",
        "\n",
        "Now, you need to implement the same MLP structure using PyTorch library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6H_WRy2Z_7z"
      },
      "source": [
        "## 3.1 MLP_torch class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ZMGmeH_kl8xT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the MLP architecture\n",
        "class MLP_torch(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP_torch, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP\n",
        "        \n",
        "        Parameters:\n",
        "            x (torch.Tensor): Input tensor\n",
        "        \n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.hidden_layer(x))\n",
        "        x = torch.softmax(self.output_layer(x), dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffTadGRvaEGr"
      },
      "source": [
        "## 3.2 Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "rlFwrpZqMr5g"
      },
      "outputs": [],
      "source": [
        "def data_preprocessing_torch(file_path):\n",
        "    # Preprocess the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    labels = df.iloc[:, 0]\n",
        "    features = df.iloc[:, 1:]\n",
        "\n",
        "    # One-hot encode the labels for classification\n",
        "    lb = LabelBinarizer()\n",
        "    one_hot_labels = lb.fit_transform(labels)\n",
        "\n",
        "    # Convert data to PyTorch tensors\n",
        "    data_tensor = torch.tensor(features.values, dtype=torch.float32) / 255  # Normalize pixel values\n",
        "    label_tensor = torch.tensor(one_hot_labels, dtype=torch.float32)\n",
        "\n",
        "    return data_tensor, label_tensor\n",
        "\n",
        "# Load the data\n",
        "# train_data_path = \"train_data.csv\"\n",
        "# data_preprocessing_torch(train_data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-jc1EsWaG33"
      },
      "source": [
        "## 3.3 Training Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "k7-fJsSwmmQm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train_torch(mlp, inputs, targets, epochs, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(mlp.parameters(), lr=learning_rate)\n",
        "    loss_curve = []\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        epoch_loss = 0\n",
        "        # Forward pass - with mlp.forward\n",
        "        outs = mlp.forward(inputs)\n",
        "        # Calculate Loss - with criterion (CrossEntropyLoss)\n",
        "        loss = criterion(outs, targets) / 10 # Hidden size\n",
        "        epoch_loss += loss.item()\n",
        "        # Backward pass - Compute gradients for example\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print epoch loss here if desired\n",
        "        print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
        "        loss_curve.append(epoch_loss)\n",
        "\n",
        "    return loss_curve\n",
        "\n",
        "# # Load the data\n",
        "# train_data_path = \"train_data.csv\"\n",
        "# test_data_path = \"test_data.csv\"\n",
        "# train_data, train_labels = data_preprocessing_torch(train_data_path)\n",
        "# test_data, test_labels = data_preprocessing_torch(test_data_path)\n",
        "\n",
        "# # Initialize the MLP with input, hidden, and output layers\n",
        "# input_size = 28 * 28\n",
        "# hidden_size = 10\n",
        "# output_size = 10\n",
        "# mlp_torch = MLP_torch(input_size, hidden_size, output_size)\n",
        "\n",
        "# # Train the MLP using PyTorch\n",
        "# epochs = 100\n",
        "# learning_rate = 0.1\n",
        "# list = train_torch(mlp_torch, train_data, train_labels, epochs, learning_rate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBFAMdOxaLwx"
      },
      "source": [
        "## 3.4 Testing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5YxTvzVPWx0i"
      },
      "outputs": [],
      "source": [
        "def test_torch(mlp, inputs, targets):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    with torch.no_grad():\n",
        "        outputs = mlp(inputs)\n",
        "        _, targets_indices = torch.max(targets, 1)\n",
        "        loss = criterion(outputs, targets_indices)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct = (predicted == targets_indices).sum().item()\n",
        "        accuracy = correct / len(inputs)\n",
        "\n",
        "    print(f\"Test Loss: {loss.item()}\")\n",
        "    print(f\"Test Accuracy: {accuracy * 100:.3f}%\")\n",
        "\n",
        "    return loss.item(), accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfHRCo-aOGv"
      },
      "source": [
        "## 3.5 Main code for PyTorch implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "lfz9GMLhMr5g"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EPOCH: 0, Loss: 0.23024067282676697\n",
            "EPOCH: 1, Loss: 0.22744961082935333\n",
            "EPOCH: 2, Loss: 0.22149555385112762\n",
            "EPOCH: 3, Loss: 0.21462073922157288\n",
            "EPOCH: 4, Loss: 0.20714084804058075\n",
            "EPOCH: 5, Loss: 0.20021584630012512\n",
            "EPOCH: 6, Loss: 0.19451218843460083\n",
            "EPOCH: 7, Loss: 0.1900450736284256\n",
            "EPOCH: 8, Loss: 0.1864955723285675\n",
            "EPOCH: 9, Loss: 0.18396608531475067\n",
            "EPOCH: 10, Loss: 0.18169744312763214\n",
            "EPOCH: 11, Loss: 0.17893081903457642\n",
            "EPOCH: 12, Loss: 0.1766240894794464\n",
            "EPOCH: 13, Loss: 0.17480109632015228\n",
            "EPOCH: 14, Loss: 0.17362555861473083\n",
            "EPOCH: 15, Loss: 0.17283916473388672\n",
            "EPOCH: 16, Loss: 0.17150233685970306\n",
            "EPOCH: 17, Loss: 0.1705017387866974\n",
            "EPOCH: 18, Loss: 0.16980648040771484\n",
            "EPOCH: 19, Loss: 0.1691531389951706\n",
            "EPOCH: 20, Loss: 0.16848944127559662\n",
            "EPOCH: 21, Loss: 0.16781993210315704\n",
            "EPOCH: 22, Loss: 0.16727712750434875\n",
            "EPOCH: 23, Loss: 0.16689318418502808\n",
            "EPOCH: 24, Loss: 0.16650453209877014\n",
            "EPOCH: 25, Loss: 0.16607950627803802\n",
            "EPOCH: 26, Loss: 0.1657426506280899\n",
            "EPOCH: 27, Loss: 0.16550052165985107\n",
            "EPOCH: 28, Loss: 0.16525566577911377\n",
            "EPOCH: 29, Loss: 0.1649875044822693\n",
            "EPOCH: 30, Loss: 0.1647394895553589\n",
            "EPOCH: 31, Loss: 0.16453814506530762\n",
            "EPOCH: 32, Loss: 0.1643809974193573\n",
            "EPOCH: 33, Loss: 0.16424351930618286\n",
            "EPOCH: 34, Loss: 0.1640942543745041\n",
            "EPOCH: 35, Loss: 0.16392485797405243\n",
            "EPOCH: 36, Loss: 0.1637590229511261\n",
            "EPOCH: 37, Loss: 0.1636315882205963\n",
            "EPOCH: 38, Loss: 0.1635477989912033\n",
            "EPOCH: 39, Loss: 0.1634690761566162\n",
            "EPOCH: 40, Loss: 0.16335894167423248\n",
            "EPOCH: 41, Loss: 0.16322870552539825\n",
            "EPOCH: 42, Loss: 0.1631176471710205\n",
            "EPOCH: 43, Loss: 0.1630408763885498\n",
            "EPOCH: 44, Loss: 0.16297639906406403\n",
            "EPOCH: 45, Loss: 0.16290012001991272\n",
            "EPOCH: 46, Loss: 0.16281379759311676\n",
            "EPOCH: 47, Loss: 0.16273081302642822\n",
            "EPOCH: 48, Loss: 0.1626538336277008\n",
            "EPOCH: 49, Loss: 0.1625811606645584\n",
            "EPOCH: 50, Loss: 0.16251710057258606\n",
            "EPOCH: 51, Loss: 0.16246050596237183\n",
            "EPOCH: 52, Loss: 0.16239993274211884\n",
            "EPOCH: 53, Loss: 0.16233165562152863\n",
            "EPOCH: 54, Loss: 0.16226664185523987\n",
            "EPOCH: 55, Loss: 0.16221199929714203\n",
            "EPOCH: 56, Loss: 0.16216200590133667\n",
            "EPOCH: 57, Loss: 0.16211020946502686\n",
            "EPOCH: 58, Loss: 0.1620573103427887\n",
            "EPOCH: 59, Loss: 0.16200502216815948\n",
            "EPOCH: 60, Loss: 0.16195368766784668\n",
            "EPOCH: 61, Loss: 0.16190513968467712\n",
            "EPOCH: 62, Loss: 0.16186004877090454\n",
            "EPOCH: 63, Loss: 0.16181513667106628\n",
            "EPOCH: 64, Loss: 0.1617678850889206\n",
            "EPOCH: 65, Loss: 0.16172125935554504\n",
            "EPOCH: 66, Loss: 0.16167797148227692\n",
            "EPOCH: 67, Loss: 0.161636084318161\n",
            "EPOCH: 68, Loss: 0.16159315407276154\n",
            "EPOCH: 69, Loss: 0.16154992580413818\n",
            "EPOCH: 70, Loss: 0.16150811314582825\n",
            "EPOCH: 71, Loss: 0.16146746277809143\n",
            "EPOCH: 72, Loss: 0.16142791509628296\n",
            "EPOCH: 73, Loss: 0.1613890826702118\n",
            "EPOCH: 74, Loss: 0.16135002672672272\n",
            "EPOCH: 75, Loss: 0.1613112837076187\n",
            "EPOCH: 76, Loss: 0.16127413511276245\n",
            "EPOCH: 77, Loss: 0.16123825311660767\n",
            "EPOCH: 78, Loss: 0.1612022966146469\n",
            "EPOCH: 79, Loss: 0.16116613149642944\n",
            "EPOCH: 80, Loss: 0.16113074123859406\n",
            "EPOCH: 81, Loss: 0.1610959768295288\n",
            "EPOCH: 82, Loss: 0.16106154024600983\n",
            "EPOCH: 83, Loss: 0.16102783381938934\n",
            "EPOCH: 84, Loss: 0.16099463403224945\n",
            "EPOCH: 85, Loss: 0.16096119582653046\n",
            "EPOCH: 86, Loss: 0.16092821955680847\n",
            "EPOCH: 87, Loss: 0.1608964055776596\n",
            "EPOCH: 88, Loss: 0.16086514294147491\n",
            "EPOCH: 89, Loss: 0.16083413362503052\n",
            "EPOCH: 90, Loss: 0.16080331802368164\n",
            "EPOCH: 91, Loss: 0.1607726365327835\n",
            "EPOCH: 92, Loss: 0.16074219346046448\n",
            "EPOCH: 93, Loss: 0.16071203351020813\n",
            "EPOCH: 94, Loss: 0.1606818288564682\n",
            "EPOCH: 95, Loss: 0.16065171360969543\n",
            "EPOCH: 96, Loss: 0.16062243282794952\n",
            "EPOCH: 97, Loss: 0.16059362888336182\n",
            "EPOCH: 98, Loss: 0.16056492924690247\n",
            "EPOCH: 99, Loss: 0.16053615510463715\n",
            "Test Loss: 1.6192744970321655\n",
            "Test Accuracy: 84.478%\n"
          ]
        }
      ],
      "source": [
        "# Load the data\n",
        "train_data_path = \"train_data.csv\"\n",
        "test_data_path = \"test_data.csv\"\n",
        "train_data, train_labels = data_preprocessing_torch(train_data_path)\n",
        "test_data, test_labels = data_preprocessing_torch(test_data_path)\n",
        "\n",
        "# Initialize the MLP with input, hidden, and output layers\n",
        "input_size = 28 * 28\n",
        "hidden_size = 30\n",
        "output_size = 10\n",
        "mlp_torch = MLP_torch(input_size, hidden_size, output_size)\n",
        "\n",
        "# Train the PyTorch model\n",
        "epochs = 100\n",
        "learning_rate = 0.01\n",
        "train_torch(mlp_torch, train_data, train_labels, epochs, learning_rate)\n",
        "\n",
        "# Test the PyTorch model\n",
        "test_loss, test_accuracy = test_torch(mlp_torch, test_data, test_labels)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Zaw1gxIGOVT9",
        "ij6-IOwKOYEw",
        "coWq1h40OhFz",
        "PeIxz7MXOjUo",
        "XDp86vtgOnH8",
        "SZIOTmN-OxHS"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
